---
title: "Tanzania People and Wildlife: Data Exploration and NDVI Time Series"
author: "Guy Lomax"
date: '2023-03-31'
output:
  html_document:
    df_print: paged
---

## Introduction

In this notebook, I generate smooth, high-resolution NDVI time series for
Tanzania People and Wildlife rangeland monitoring plots from 2017-2022. 

Sentinel-2 provides global satellite imagery at 5-day intervals and at
10-meter resolution, making it one of the most precise freely available
satellite datasets currently available. Sentinel-2 imagery at 5-day
intervals is available for East Africa from early 2017, providing about
six years of continuous data.

The fine resolution of Sentinel-2 allows us to understand vegetation
patterns and change on smaller scales than previously possible, to more
easily distinguish between open areas and tree- or shrub-covered areas,
and to connect remote sensing data to field data collected at the level
of individual sample plots or transects.

Here, I have used the cloud-based platform Google Earth Engine to
extract raw Sentinel-2 data from September 2017 to September 2022 for
each of the TPW monitoring plots, and convert the raw data into a
continuous NDVI time series that provide detailed insight into the
patterns of vegetation productivity and change at TPW monitoring plots.

```{r setup, include = F}

# Analysis
library(signal)
library(imputeTS)

# Data management
library(here)
library(tidyverse)
library(lubridate)
library(sf)

# Visualisation
library(ggthemes)
library(tmap)

tmap_mode("view")

knitr::opts_chunk$set(
  comment = "#>",
  echo = FALSE, warning = FALSE, message = FALSE,
  fig.keep = "all", fig.width = 8
)

```

## Load data and extract transect locations

TPW's original rangeland monitoring data were in the form of a layer in ArcPro.
I have converted the data into a Shapefile and a CSV to load into R.

```{r load}

# TPW data
tpw_points <- st_read(here("data", "raw", "vector", "tpw",
                           "Rangeland_Ufuatiliaji_wa_Nyanda_za_Malisho.shp"))
tpw_table <- read_csv(here("data", "raw", "csv", "tpw", "samplemetrics.csv"))

# Tanzania country boundary
tz <- st_read(here("data", "raw", "vector", "natural_earth",
                   "ne_110m_admin_0_countries_fixed.shp")) %>%
  filter(NAME == "Tanzania") %>%
  select(NAME)

```

The original data contains `r nrow(tpw_points)` rows, each representing
a sampling event of one of `r length(unique(tpw_points$village_pl))` TPW
rangeland monitoring plots.

For now, I clean the data by removing sampling events that have fewer or
more than 20 associated measurements along the transect, as well as
those with greater than 100 total strikes reported for bare + basal
ground cover (they should sum to no more than 100).

Extracting the coordinates of the transects is challenging because each
sampling event has its own coordinates, which sometimes show more than
one location for a single plot.

To ensure locations are accurate, I have removed plots with less than 12 months
of data, and then removed those for which more than half of the recorded locations are
more than 50m from the median location. For the remaining points (148 plots),
I have assumed the median location reflects the true centre of the plot.

```{r join_clean}

tpw_joined <- tpw_points %>%
  mutate(globalid = toupper(globalid)) %>%
  left_join(tpw_table, by = c("globalid" = "parentglobalid"))

# Columns to keep
vars <- c("globalid", "plot_id", "village", "dateTime",
          "plotburned", "plotgrazed", "plotcolor", "invasives_", "plotqualit",
          "sampleID", "markbare", "markbasal", "grassheight")

tpw_condensed <- tpw_joined %>%
  unite("plot_id", starts_with("plotID"),
        sep = "", na.rm = TRUE) %>%
  select(all_of(vars)) %>%
  group_by(globalid, plot_id, village, dateTime) %>%
  summarise(n = n(),
            plotburned = first(plotburned),
            plotgrazed = first(plotgrazed),
            plotcolor = first(plotcolor),
            invasives = first(invasives_),
            plotquality = first(plotqualit),
            bare = sum(markbare),
            basal = sum(markbasal),
            grassheight = mean(grassheight)) %>%
  ungroup() %>%
  select(-globalid)

# Remove rows with no data or incorrect numbers of samples
# I could include the 8 rows with 21/22 samples by manually removing duplicates
# Also remove locations outside Tanzania
# Also remove locations with < 12 valid observations

tpw_clean <- tpw_condensed %>%
  filter(n == 20) %>%
  filter(!is.na(plotburned) & !is.na(plotgrazed)) %>%
  filter(bare + basal == 100) %>%
  st_filter(tz) %>%
  group_by(plot_id) %>%
  filter(n() >= 12)

# Extract locations and remove those with more than 50% of metrics more than
# 50m from median point (assuming 100m transect/buffer)
tpw_locations <- tpw_clean %>%
  mutate(x = st_coordinates(geometry)[,1],
         y = st_coordinates(geometry)[,2]) %>%
  st_drop_geometry() %>%
  group_by(plot_id) %>%
  mutate(med_x = median(x),
         med_y = median(y),
         dist = sqrt((x - med_x) ^ 2 + (y - med_y) ^ 2)) %>%
  filter(sum(dist < 0.00045) / n() > 0.5) %>%
  summarise(x = first(med_x), y = first(med_y)) %>%
  st_as_sf(crs = 4326, coords = c("x", "y"))

tpw_locations_fixed <- tpw_clean %>%
  st_drop_geometry %>%
  left_join(tpw_locations)

# Visualise tpw_locations on map of Tanzania

tmap_leaflet(tm_shape(tpw_locations) + tm_dots(size = 0.02, col = "brown") +
  tm_view(set.view = 8) + tm_basemap(leaflet::providers$Stamen.Terrain))

# Write locations to shapefile for extraction

# st_write(tpw_locations,
#          here("data", "processed", "vector", "tpw", "tpw_locations.shp"),
#          delete_dsn = T)


```

I then exported these locations to be used in Google Earth Engine to
extract Sentinel-2 vegetation index values for the plots.

## Combine with Sentinel-2 NDVI time series data from Google Earth Engine

The GPS coordinates of the transect locations were used to extract local
NDVI (Normalised Difference Vegetation Index) and MSAVI2 (Modified
Soil-Adjusted Vegetation Index) values for all Sentinel-2 images
intersecting the location from 2017-2022 at 10m resolution. I extracted the mean
values across a circle of 50m radius centred on the plot location, excluding
pixels identified as tree, shrub or other non-grassy land cover.

I have randomly selected six of the monitoring transect locations to
visualise the NDVI time series.

```{r full_ts, cache = T}

# Load and clean NDVI time series data from GEE
# Replace nodata value (-100) with NA
# Convert band values for partially masked plots to include unmasked values only
# Rename
# Exclude pixels within wet season lake boundaries

tpw_ts_all <- read_csv(here("data", "processed", "csv", "tpw",
                            "tpwTSValues_all_l1c.csv")) %>%
  select(-"system:index", -".geo") %>%
  na_if(-100) %>%
  mutate(across(.cols = c("B2", "B3", "B4", "B8", "ndvi", "msavi"),
                .fns = function(x) (x + 100 * maskFrac) / (1 - maskFrac))) %>%
  rename(blue = B2,
         green = B3,
         red = B4,
         ir = B8,
         lc = Map) %>%
  filter(lc != 80)

# Merge double readings (more than one value in a day) by taking higher ndvi
# value or lower raw band value

# Import function s() from hablar package - manages NA values intuitively

s <- hablar::s

tpw_ts_unique <- tpw_ts_all %>%
  group_by(plot_id, imgDate) %>%
  summarise(blue = min(s(blue)), green = min(s(green)),
            red = min(s(red)), ir = min(s(ir)),
            ndvi = max(s(ndvi)), msavi = max(s(msavi)),
            lc = first(lc), precipitation = first(precipitation)) %>%
  ungroup()

# Remove plots with more than 90% NA values

tpw_ts_unique <- tpw_ts_unique %>%
  group_by(plot_id) %>%
  filter(sum(is.na(ndvi)) < 0.9 * n())

# Plot sample
set.seed(14941)
plot_names <- unique(tpw_ts_unique$plot_id)
sample_plots <- sample(plot_names, 6)

tpw_ts_unique %>%
  filter(plot_id %in% sample_plots) %>%
  ggplot(aes(x = imgDate, y = ndvi)) +
  facet_wrap(~plot_id) +
  # geom_line() +
  geom_point(size = 0.5) +
  theme_clean() +
  labs(x = "Date", y = "NDVI")


```

As can be seen, the raw time series contains noise and many missing
values due to cloud cover and variable atmospheric conditions. We can
fill these gaps using an intelligent smoothing approach to create a
continuous time series that approximates the true time series values
(based on Chen et al. 2004). The method relies on two facts: (a) Real
NDVI tends to change smoothly, without abrupt spikes or drops. (b) Cloud
cover and other disturbances tend to reduce NDVI values compared to
their true values.

This means that we can make a best guess of the "true" value by fitting
a curve to the upper envelope of the time series. To do this we use a
Savitzky-Golay filter, which smooths out the jagged noise in the time
series, and use it to boost unusually low values in the time series
towards the upper envelope.

```{r gapfilling, cache = T, dependson= "full_ts"}

# Apply Chen et al. 2004 Savitzky-Golay-based gapfilling and smoothing algorithm
# to remove noise and cloud gaps

# Conduct a linear interpolation to create a complete time series
# Then remove points with prior decrease or subsequent increase of > 0.1 in
# 5 days (assume cloud)
# Then fill these gaps with another linear interpolation

tpw_all_int <- tpw_ts_unique %>%
  group_by(plot_id) %>%
  arrange(imgDate) %>%
  mutate(ndvi_int = na_interpolation(ndvi),
         ndvi_spike = (lead(ndvi_int) - ndvi_int > 0.1),
         ndvi_drop = (lag(ndvi_int) - ndvi_int > 0.1),
         ndvi_int = na_if(ndvi_int, ((ndvi_spike | ndvi_drop) * ndvi)),
         ndvi_int = na_interpolation(ndvi_int)) %>%
  select(-ndvi_spike, -ndvi_drop)
         

# Apply Savitzky-Golay filter to smooth signal
# For now, use m = 40-60 days (i.e., 8-12 images) and d = 3
# Will go back and code up internal optimisation of these values later

# sg_params <- tibble(p = rep(2:4, 7),
#                     m = rep(8:14, each = 3))
# 
# # Find optimal smoothing values for each time series
# find_least_error <- function(ts) {
#   # Calculate sum of squared errors for each combination of parameters
#   sse <- numeric(length = nrow(sg_params))
#   
#   for (i in seq_len(nrow(sg_params))) {
#     p <- sg_params$p[i]
#     m <- sg_params$m[i]
#     
#     new_ts <- sgolayfilt(ts, p = p, n = 2 * m + 1)
#     
#     sse[i] <- sum((ts - new_ts) ^ 2)
#   }
#   
#   # Choose iteration which has minimum sse
#   opt <- which.min(sse)
#   opt
# }

# tpw_sg1_opt <- tpw_all_int %>%
#   group_by(plot_id) %>%
#   mutate(opt = find_least_error(ndvi_int))

tpw_sg1 <- tpw_all_int %>%
  group_by(plot_id) %>%
  mutate(ndvi_filtered = sgolayfilt(ndvi_int, p = 3, n = 25))

# Assign weights to NDVI values in original (interpolated) series based on
# whether they fall above or below the trend curve for that TS.

tpw_weights <- tpw_sg1 %>%
  mutate(dist = ndvi_filtered - ndvi_int,
         max_dist = max(dist),
         weight = ifelse(dist > 0, 1 - (dist / max_dist), 1))

# Iterative approach to upper ndvi envelope
# Generate new time series by replacing "noisy" NDVI values with filtered ones
# Second, shorter-period SG filter

iterative_fit <- function(ndvi_1, ndvi_0, weight) {
  
  initial_fit <- 100
  new_fit <- sum(abs(ndvi_1 - ndvi_0) * weight)
  ndvi_new <- ndvi_1

  while(initial_fit > new_fit) {
    initial_fit <- new_fit
    
    ndvi_new <- ifelse(ndvi_new >= ndvi_0, ndvi_new, ndvi_0) %>%
    sgolayfilt(p = 4, n = 15)
  
  new_fit <- sum(abs(ndvi_new - ndvi_0) * weight)
  }
  ndvi_new
}

# Iterate to get final time series

tpw_final <- tpw_weights %>%
  mutate(ndvi_new = iterative_fit(ndvi_filtered, ndvi_int, weight)) %>%
  select(-dist, -max_dist, -weight, -ndvi_int, -ndvi_filtered)

# tpw_final %>%
#   filter(plot_id %in% sample_plots) %>%
#   pivot_longer(cols = c("ndvi_int", "ndvi_filtered", "ndvi_new"),
#                names_to = "var", values_to = "value") %>%
#   ggplot() +
#   geom_line(aes(x = imgDate, y = value, colour = var)) +
#   facet_wrap(~plot_id) +
#   theme_bw() +
#   scale_x_date(breaks = "6 months", minor_breaks = "3 month") +
#   ylim(0, 1) +
#   theme(legend.position="none",
#         axis.text.x = element_text(angle = 60, vjust = 0.5, hjust=1))

tpw_final %>%
  filter(plot_id %in% sample_plots) %>%
  ggplot(aes(x = imgDate)) +
  # geom_col(aes(y = precipitation / max(precipitation)), fill = "blue") +
  geom_line(aes(y = ndvi_new), colour = "green", size = 1) +
  geom_point(aes(y = ndvi), colour = "grey40", size = 0.5) +
  # geom_line(aes(y = ndvi), colour = "grey40") +
  facet_wrap(~plot_id) +
  theme_clean() +
  # scale_x_date(breaks = "6 months", minor_breaks = "3 month") +
  ylim(0, 1) +
  # theme(axis.text.x = element_text(angle = 60, vjust = 0.5, hjust=1)) +
  labs(x = "Date", y = "NDVI")

```

The grey shows the original data, while the green shows the smoothed
version. It isn't perfect - it still tends to give a spiky pattern when
there is a lot of noise or not much data (e.g., in Lembiti) - but it
is a big improvement on the original data. I am continuing to refine
this.

The annual seasonal cycle is very clearly visible, and you can clearly
see the differences between sites like Mnyambaa and those like Norporokwa.
The smoother also preserves the overall shape of the increase at the
start of each wet season and the subsequent decrease.

## Extracting vegetation metrics from satellite data

From these curves, we can start to extract some potentially useful
metrics to track land health, degradation and resilience. A few we might
be interested in are:

1.  The peak greenness each year compared to baseline.
2.  The overall area below the curve, a proxy for productivity.
3.  The relationship between these values and seasonal rainfall.

```{r metrics, cache = T, dependson = c("full_ts", "gapfilling")}

tpw_hyear <- tpw_final %>%
  mutate(hyear = year(imgDate + dmonths(4))) %>%
  filter(hyear > 2016)

tpw_metrics <- tpw_hyear %>%
  group_by(plot_id) %>%
  mutate(min_ndvi = min(ndvi_new),
         ndvi_pc10 = quantile(ndvi_new, 0.1),
         growing_season = ndvi_new > ndvi_pc10) %>%
  group_by(plot_id, hyear) %>%
  summarise(min_ndvi = first(min_ndvi),
            seasonal_min_ndvi = min(ndvi_new),
            peak_ndvi = max(ndvi_new) - min_ndvi,
            ndvi_auc = sum(ndvi_new * growing_season) - 
              (sum(growing_season) * min_ndvi),
            season_ppt = sum(precipitation * growing_season))

tpw_hyear %>%
  filter(plot_id %in% sample_plots) %>%
  group_by(plot_id) %>%
  mutate(ppt_scaled = precipitation / max(precipitation, na.rm = T)) %>%
  ggplot(aes(x = imgDate)) +
  geom_line(aes(y = ndvi_new), colour = "green") +
  geom_point(aes(y = ndvi), colour = "grey", size = 0.2) +
  geom_col(aes(y = ppt_scaled), alpha = 0.5) +
  facet_wrap(~plot_id) +
  ylim(0, 1) +
  theme_clean() +
  labs(x = "Date", y = "NDVI",
       title = "NDVI relative to rainfall timing and amount",
       subtitle = "Columns represent dates of rainfall")

tpw_metrics %>%
  filter(plot_id %in% sample_plots) %>%
  ggplot(aes(x = season_ppt, y = ndvi_auc, colour = plot_id, shape = plot_id)) +
  geom_point(size = 2) +
  theme_clean() +
  labs(x = "Seasonal rainfall (mm)",
       y = "Integrated seasonal NDVI",
       colour = "Plot",
       shape = "Plot")

```

Another indicator of interest may be the recovery rate and decay rate of
NDVI at the start or end of the season. More rapid recovery and greater
persistence of green vegetation in the dry season may be associated with
greater vegetation resilience and the presence of perennial grasses and
herbs in addition to annual species.

We can quantify this for each site by fitting growth and decay curves to
the NDVI profile for each year.

In the section below, the first figure shows an example of models fitted
to the growth phase (start of the growing season) and the decay phase
(end of the growing season). In some cases, the model hasn't been able
to fit the shape of the curve, because the shape is irregular.

Fitting these statistical models to the data allows us to extract the
growth rate and decay rate of each season as a single number. We can
then use these growth rates to explore how plots are changing over time,
and examine whether this is driven by rainfall or other factors (like
change in vegetation). The second figure shows the distribution of
growth rates and decay rates for all the plots over the period
2017-2022.

```{r growth_decay_models, cache = T,dependson = c("full_ts", "gapfilling", "metrics")}

# Take first and second derivatives to identify maxima and minima in time series
# First smooth with cubic splines to reduce noise in derivatives
tpw_derivatives <- tpw_hyear %>%
  select(plot_id, hyear, imgDate, ndvi, ndvi_new) %>%
  group_by(plot_id) %>%
  arrange(imgDate) %>%
  mutate(ndvi_smooth = smooth.spline(imgDate, ndvi_new)$y) %>%
  mutate(ndvi_1d = (lag(ndvi_new) - lead(ndvi_new)) / as.numeric(lag(imgDate) - lead(imgDate)),
         ndvi_1ds = (lag(ndvi_smooth) - lead(ndvi_smooth)) / as.numeric(lag(imgDate) - lead(imgDate)),
         ndvi_2d = (lag(ndvi_1d) - lead(ndvi_1d)) / as.numeric(lag(imgDate) - lead(imgDate)),
         ndvi_2ds = (lag(ndvi_1ds) - lead(ndvi_1ds)) / as.numeric(lag(imgDate) - lead(imgDate)))

# Identify points where 1st derivative == 0 as maxima (2nd derivative < 0) or
# minima (2nd derivative > 0)
# Since 1st derivative won't be exactly zero in discrete time series, look for
# where 1st derivative undergoes a change in sign

tpw_max_min <- tpw_derivatives %>%
  mutate(is_max = (ndvi_1ds * lag(ndvi_1ds) < 0) & (ndvi_2ds < 0),
         is_min = (ndvi_1ds * lag(ndvi_1ds) < 0) & (ndvi_2ds > 0))

# For each hydrological year, identify the greatest increase and decrease
# between subsequent maxima/minima

# 1. Create data frame of only max and min rows (including NDVI, imgDate and
#    plot_id) arranged by imgDate
# 2. Calculate NDVI differences between rows
# 3. Extract imgDate for min value (largest drop =, i.e., decay) and max value
#    (largest increase, i.e., recovery)
#
# To ensure segments are correctly identified within an hyear, we first find the
# maxima (end of growth segment and start of decay) and then find the minima
# before and after, respectively

tpw_events <- tpw_max_min %>%
  filter(is_max | is_min) %>%
  select(plot_id, imgDate, hyear, ndvi_new, ndvi_smooth, is_max, is_min) %>%
  mutate(lead_diff = lead(ndvi_new) - ndvi_new,
         lag_diff = lag(ndvi_new) - ndvi_new,
         next_imgDate = lead(imgDate),
         prev_imgDate = lag(imgDate)) %>%
  group_by(plot_id, hyear) %>%
  summarise(growth_start = prev_imgDate[which.min(lag_diff)],
            growth_end = imgDate[which.min(lag_diff)],
            decay_start = imgDate[which.min(lead_diff)],
            decay_end = next_imgDate[which.min(lead_diff)])

# Join with original time series for fitting
tpw_fitting <- tpw_hyear %>%
  left_join(tpw_events) %>%
  group_by(plot_id, hyear) %>%
  mutate(ndvi_scaled = ndvi_new - min(ndvi_new)) %>%
  mutate(growth_leg = (imgDate >= growth_start & imgDate <= growth_end),
         decay_leg = (imgDate >= decay_start & imgDate <= decay_end))

# Define functions to fit growth and decay legs

growth_fit <- function(ts, date, model = "linear") {
  df <- tibble(date, ts) %>%
    mutate(date = as.numeric(date),
           date_scaled = (date - min(date)) / (max(date) - min(date)))
  
  if(model == "linear") {
    
    mod <- tryCatch(
      lm(ts ~ date_scaled, data = df),
      error = function(err) NA
    )
    
  } else if(model == "logistic") {
    
    mod <- tryCatch(
      nls(ts~K/(1+exp(-r*(date_scaled - x0))), data=df,
          start=list(K=max(df$ts), r=5, x0 = 0.5),
          control=list(maxiter=2000, minFactor=.00000000001)),
      error=function(err) NA
    )
    
  } else if(model == "exponential") {
    
    mod <- tryCatch(
      nls(ts ~ a * exp(r * date_scaled), data = df,
          start = list(a = 0.05, r = 2),
          control=list(maxiter=2000, minFactor=.00000000001)),
      error = function(err) NA
    )
  } else {
    
    stop("Invalid model name. Choose one of 'linear', 'logistic' or 'exponential'")
    
  }
}

decay_fit <- function(ts, date, model = "linear") {
  df <- tibble(date, ts) %>%
    mutate(date = as.numeric(date),
           date_scaled = (date - min(date)) / (max(date) - min(date)))
  
  if(model == "linear") {
    
    mod <- tryCatch(
      lm(ts ~ date_scaled, data = df),
      error = function(err) NA
    )
    
  } else if(model == "logistic") {
    
    mod <- tryCatch(
      nls(ts~K/(1+exp(-r*(date_scaled - x0))), data=df,
          start=list(K=max(df$ts), r=-5, x0 = 0.5),
          control=list(maxiter=2000, minFactor=.00000000001)),
      error=function(err) NA
    )
    
  } else if(model == "exponential") {
    
    mod <- tryCatch(
      nls(ts ~ a * exp(r * date_scaled), data = df,
          start = list(a = max(ts), r = -2),
          control=list(maxiter=2000, minFactor=.00000000001)),
      error = function(err) NA
    )
  } else {
    
    stop("Invalid model name. Choose one of 'linear', 'logistic' or 'exponential'")
    
  }
}

# Apply functions to each site and hydrological year
t1 <- Sys.time()
tpw_growth_model <- tpw_fitting %>%
  filter(growth_leg) %>%
  summarise(mod = list(growth_fit(ndvi_scaled, imgDate, "logistic")))
t2 <- Sys.time()
cat("Fitting time - growth: ", t2 - t1)

t1 <- Sys.time()
tpw_decay_model <- tpw_fitting %>%
  filter(decay_leg) %>%
  summarise(mod = list(decay_fit(ndvi_scaled, imgDate, "logistic")))
t2 <- Sys.time()
cat("Fitting time - decay: ", t2 - t1)

# Test overall fit
tidy <- function(mod) {
  if (is.logical(mod)) {
    NA
  } else {
    broom::tidy(mod)
  }
}
growth_params <- tpw_growth_model %>%
  mutate(params = map(mod, tidy)) %>%
  unnest(params) %>%
  mutate(pc_error = std.error / estimate * 100)

growth_performance <- tpw_growth_model %>%
  mutate(glance = tryCatch(map(mod, broom::glance), error = function(err) NA)) %>%
  unnest(glance)

decay_params <- tpw_decay_model %>%
  mutate(params = map(mod, tidy)) %>%
  unnest(params) %>%
  mutate(pc_error = std.error / estimate * 100)

decay_performance <- tpw_decay_model %>%
  mutate(glance = tryCatch(map(mod, broom::glance), error = function(err) NA)) %>%
  unnest(glance)

# mean(growth_performance$sigma)
# median(growth_performance$sigma)
# mean(decay_performance$sigma)
# median(decay_performance$sigma)
# 
# sum(is.na(tpw_growth_model$mod))
# sum(is.na(tpw_decay_model$mod))

```

```{r model_vis}

# Extract model fitted values
augment <- function(mod) {
  if (is.logical(mod)) {
    NA
  } else {
    broom::augment(mod)
  }
}

growth_fitted <- tpw_growth_model %>%
  mutate(values = map(mod, augment)) %>%
  unnest(values) %>%
  select(-mod, -date_scaled) %>%
  rename(fitted_growth = .fitted,
         resids_growth = .resid)

decay_fitted <- tpw_decay_model %>%
  mutate(values = map(mod, augment)) %>%
  unnest(values) %>%
  select(-mod, -date_scaled) %>%
  rename(fitted_decay = .fitted,
         resids_decay = .resid)

# Combine growth and decay fitted values with original data frame

tpw_fitted <- tpw_fitting %>%
  ungroup() %>%
  full_join(growth_fitted, by = c("plot_id", "hyear", "ndvi_scaled" = "ts")) %>%
  full_join(decay_fitted, by = c("plot_id", "hyear", "ndvi_scaled" = "ts")) %>%
  select(-starts_with("growth"), -starts_with("decay")) %>%
  rename(growth = fitted_growth, decay = fitted_decay) %>%
  pivot_longer(cols = c(growth, decay),
               names_to = "leg", values_to = "fitted")

# Visualise
tpw_fitted %>%
  filter(plot_id %in% sample_plots) %>%
  ggplot(aes(x = imgDate)) +
  geom_line(aes(y = ndvi_scaled)) +
  geom_line(aes(y = fitted, colour = leg), size = 1.5, alpha = 0.8) +
  scale_colour_brewer(palette = "Dark2", direction = -1) +
  facet_wrap(~plot_id) +
  theme_bw() +
  ylim(-0.1, 1)

# Visualise growth/decay rate and growth/decay rate divided by ceiling

all_params <- bind_rows("growth" = growth_params, "decay" = decay_params,
                        .id = "leg") %>%
  pivot_wider(id_cols = c("plot_id", "hyear", "term", "leg"),
              names_from = term, values_from = estimate)

all_params %>%
  filter((leg == "growth" & r > 0) | (leg == "decay" & r < 0)) %>%
  ggplot(aes(x = r, fill = leg)) +
  geom_histogram(colour = "black", alpha = 0.6, binwidth = 0.5, position = "identity") +
  scale_fill_brewer(palette = "Dark2", direction = -1) +
  geom_vline(xintercept = 0) +
  theme_clean() +
  labs(x = "Growth or decay rate", y = "Count")


```


These models are not always a good fit to the data, but for the majority of
time series they provide a way of quantifying the growth and decay rates of
vegetation greenness over time.

From here, we could use these data in different ways:

1.  We could look at changes over time in greenness, in recovery/decay
    rates or in other metrics like rain use efficiency, to explore which
    plots or areas are improving or deteriorating over time.
2.  We could examine the drivers behind the differences in growth or
    decay rates, for example distinguishing the effects of rainfall,
    soil type or management.
3.  We could compare these metrics between regions with similar areas
    outside the regions that TPW works in, for example to better
    understand impact.
4.  We could compare some of these metrics to the field data TPW has
    collected, for example in order to test whether they allow us to
    predict ground cover or the presence of particular species.
