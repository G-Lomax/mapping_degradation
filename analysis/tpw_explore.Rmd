---
title: "TPW Data Exploration"
output: html_notebook
author: "Guy Lomax"
date: 2023-03-13
---

```{r setup}

# Analysis
library(signal)
library(imputeTS)

# Data management
library(here)
library(tidyverse)
library(lubridate)
library(sf)

# Visualisation
library(tmap)
library(ggthemes)


```

Load data.

Original data were in the form of a nested layer, with each point
(sampling event) accompanied by a nested table of 20 rows, one for every 5 m
along the 100 m N-S transect. Data were extracted as two separate files: a
shapefile containing 7,273 rows (one per sampling event) and a csv containing
130,102 rows (20 per sampling event or fewer in some cases).

```{r load}

tpw_points <- st_read(here("data", "raw", "vector", "tpw",
                           "Rangeland_Ufuatiliaji_wa_Nyanda_za_Malisho.shp"))
tpw_table <- read_csv(here("data", "raw", "csv", "tpw", "samplemetrics.csv"))
tz <- st_read(here("data", "raw", "vector", "natural_earth",
                   "ne_110m_admin_0_countries_fixed.shp")) %>%
  filter(NAME == "Tanzania") %>%
  select(NAME)

colnames(tpw_points)
colnames(tpw_table)

```
Join points to table, summarise and clean

```{r join_clean}

tpw_joined <- tpw_points %>%
  mutate(globalid = toupper(globalid)) %>%
  left_join(tpw_table, by = c("globalid" = "parentglobalid"))

# n_distinct(tpw_points$globalid)
# n_distinct(tpw_table$parentglobalid)

# n_measurements <- tpw_joined %>% group_by(globalid) %>% summarise(n = n())

# 1,255 out of 7,273 sampling events in tpw_points have zero rows from tpw_table
# associated with them. A further 53 rows have incorrect (not 20) images and
# may need cleaning.

# Columns to keep
vars <- c("globalid", "plot_id", "village", "village_pl", "dateTime",
          "plotburned", "plotgrazed", "plotcolor", "invasives_", "plotqualit",
          "sampleID", "markbare", "markbasal", "grassheight")

tpw_condensed <- tpw_joined %>%
  unite("plot_id", starts_with("plotID"),
        sep = "", na.rm = TRUE) %>%
  select(all_of(vars)) %>%
  group_by(globalid, plot_id, village, village_pl, dateTime) %>%
  summarise(n = n(),
            plotburned = first(plotburned),
            plotgrazed = first(plotgrazed),
            plotcolor = first(plotcolor),
            invasives = first(invasives_),
            plotquality = first(plotqualit),
            bare = sum(markbare),
            basal = sum(markbasal),
            grassheight = mean(grassheight)) %>%
  ungroup()

# Remove rows with no data or incorrect numbers of samples
# I could include the 8 rows with 21/22 samples by manually removing duplicates
# Also remove locations outside Tanzania

tpw_clean <- tpw_condensed %>%
  filter(n == 20) %>%
  filter(!is.na(plotburned) & !is.na(plotgrazed)) %>%
  filter(bare + basal == 100) %>%
  st_filter(tz)

tpw_locations <- tpw_clean %>%
  mutate(x = st_coordinates(geometry)[,1],
         y = st_coordinates(geometry)[,2]) %>%
  st_drop_geometry() %>%
  group_by(plot_id) %>%
  summarise(x = median(x),
            y = median(y)) %>%
  st_as_sf(crs = 4326, coords = c("x", "y"))

# How many are within Tanzania?
# st_filter(tpw_locations, tz) %>% nrow()
# All are within Tanzania - let's assume they're correct for now

tpw_locations_fixed <- tpw_clean %>%
  st_drop_geometry %>%
  left_join(tpw_locations)

# Write locations to shapefile for extraction

# st_write(tpw_locations,
#          here("data", "processed", "vector", "tpw", "tpw_locations.shp"),
#          delete_dsn = T)

```

Join with Sentinel 2 band and VI values and CHIRPS monthly precipitation
for each plot extracted from Google Earth Engine and explore time series.

```{r ts_eda}

tpw_bands <- read_csv(here("processed_data", "csv", "tpw",
                           "tpwMonthlyValues.csv")) %>%
  select(-"system:index", -".geo") %>%
  rename(blue = B2,
         green = B3,
         red = B4,
         ir = B8,
         lc = Map) %>%
  filter(lc != 80) %>%  # Exclude plots flooded in 2021 (Lake Manyara shoreline)
  mutate(month = month(start),  # Extract month and year for joining
         year = year(start))

# Join with plot data

tpw_complete <- tpw_locations_fixed %>%
  mutate(month = month(dateTime),
         year = year(dateTime)) %>%
  inner_join(tpw_bands)

# Explore a few time series
# Select a sample of n random plots

set.seed(14941)
n <- 6
sample_plots <- unique(tpw_complete$plot_id) %>%
  sample(n)

cols_to_pivot <- c("bare", "basal", "grassheight", "msavi", "ndvi", "precipitation")

tpw_complete %>%
  filter(plot_id %in% sample_plots) %>%
  mutate(midpoint = as_date((as.numeric(start) + as.numeric(end)) / 2)) %>%
  pivot_longer(cols = all_of(cols_to_pivot),
               names_to = "var",
               values_to = "value") %>%
  ggplot(aes(x = midpoint, y = value, colour = plot_id)) +
  geom_line() +
  facet_wrap(~var, scales = "free") +
  theme_bw()

# Look at NDVI time series only
tpw_complete %>%
  filter(plot_id %in% sample_plots) %>%
  mutate(midpoint = as_date((as.numeric(start) + as.numeric(end)) / 2)) %>%
  ggplot(aes(x = midpoint, y = ndvi, colour = plot_id)) +
  geom_line() +
  theme_bw()


```

Explore twice-monthly time series:

```{r biweekly_ts}

tpw_biweekly <- read_csv("processed_data/csv/tpw/tpwTSValues.csv") %>%
  select(-"system:index", -".geo") %>%
  rename(blue = B2,
         green = B3,
         red = B4,
         ir = B8,
         lc = Map) %>%
  filter(lc != 80)

tpw_biweekly %>%
  group_by(plot_id) %>%
  summarise(frac_na = sum(is.na(ndvi) / n())) %>%
  ggplot(aes(x = frac_na)) +
  geom_histogram(bins = 20, fill = "green", colour = "dark green", alpha = 0.5)

tpw_complete %>%
  group_by(plot_id) %>%
  summarise(frac_na = sum(is.na(ndvi) / n())) %>%
  ggplot(aes(x = frac_na)) +
  geom_histogram(bins = 20, fill = "green", colour = "dark green", alpha = 0.5)

tpw_biweekly %>%
  filter(plot_id %in% sample_plots) %>%
  mutate(midpoint = as_date((as.numeric(start) + as.numeric(end)) / 2)) %>%
  ggplot(aes(x = midpoint, y = ndvi, colour = plot_id)) +
  geom_line() +
  theme_bw()

tpw_complete %>%
  filter(plot_id %in% sample_plots) %>%
  mutate(midpoint = as_date((as.numeric(start) + as.numeric(end)) / 2)) %>%
  ggplot(aes(x = midpoint, y = ndvi, colour = plot_id)) +
  geom_line() +
  theme_bw()

```

The two-week window dataset is much more useful than the monthly resolution,
but suffers from gaps caused by cloud cover. I would like to use this (or even
finer-resolution data) but would need to implement some gap-filling method.

Possible approaches:
A: Simple linear interpolation between adjacent dates
B: A more sophisticated gap-filling method, e.g., using local pixel rankings to
estimate increases based on both local pixels and dates, as in this paper:
https://ieeexplore.ieee.org/abstract/document/8276645
C: A temporal smoother, e.g., Savitsky-Golay or Gaussian Process
D: Use Sentinel-1 or MODIS as well as prior and subsequent NDVI values
to estimate an intermediate value.

Let's first look at data taking all S2 images (not aggregated monthly/biweekly):

```{r full_ts}

tpw_ts_all <- read_csv("processed_data/csv/tpw/tpwTSValues_all.csv") %>%
  select(-"system:index", -".geo") %>%
  rename(blue = B2,
         green = B3,
         red = B4,
         ir = B8,
         lc = Map) %>%
  filter(lc != 80)

tpw_ts_all %>%
  group_by(plot_id) %>%
  summarise(frac_na = sum(is.na(ndvi) / n())) %>%
  ggplot(aes(x = frac_na)) +
  geom_histogram(bins = 20, fill = "green", colour = "dark green", alpha = 0.5)

tpw_ts_all %>%
  filter(plot_id %in% sample_plots) %>%
  ggplot(aes(x = imgDate, y = ndvi, colour = plot_id)) +
  geom_line() +
  theme_bw()




```

Simple linear interpolation:

```{r lin_interp}

# Apply Chen et al. 2004 Savitzky-Golay-based gapfilling and smoothing algorithm
# to remove noise and cloud gaps

# First remove points with subsequent increase of > 0.1 in 5 days (assume cloud)
# Then linear interpolation for all gaps
tpw_all_int <- tpw_ts_all %>%
  group_by(plot_id) %>%
  arrange(imgDate) %>%
  mutate(ndvi_spike = (lead(ndvi) - ndvi > 0.1),
         ndvi = na_if(ndvi, (ndvi_spike * ndvi)),
         ndvi_int = na_interpolation(ndvi)) %>%
  select(-ndvi_spike)
         

# Apply Savitzky-Golay filter to smooth signal
# For now, use m = 40-60 days (i.e., 8-12 images) and d = 3
# Will go back and code up internal optimisation of these values later

# sg_params <- tibble(p = rep(2:4, 7),
#                     m = rep(8:14, each = 3))
# 
# # Find optimal smoothing values for each time series
# find_least_error <- function(ts) {
#   # Calculate sum of squared errors for each combination of parameters
#   sse <- numeric(length = nrow(sg_params))
#   
#   for (i in seq_len(nrow(sg_params))) {
#     p <- sg_params$p[i]
#     m <- sg_params$m[i]
#     
#     new_ts <- sgolayfilt(ts, p = p, n = 2 * m + 1)
#     
#     sse[i] <- sum((ts - new_ts) ^ 2)
#   }
#   
#   # Choose iteration which has minimum sse
#   opt <- which.min(sse)
#   opt
# }

# tpw_sg1_opt <- tpw_all_int %>%
#   group_by(plot_id) %>%
#   mutate(opt = find_least_error(ndvi_int))

tpw_sg1 <- tpw_all_int %>%
  group_by(plot_id) %>%
  mutate(ndvi_filtered = sgolayfilt(ndvi_int, p = 3, n = 25))

# Assign weights to NDVI values in original (interpolated) series based on
# whether they fall above or below the trend curve for that TS.

tpw_weights <- tpw_sg1 %>%
  mutate(dist = ndvi_filtered - ndvi_int,
         max_dist = max(dist),
         weight = ifelse(dist > 0, 1 - (dist / max_dist), 1))

# Iterative approach to upper ndvi envelope
# Generate new time series by replacing "noisy" NDVI values with filtered ones
# Second, shorter-period SG filter

iterative_fit <- function(ndvi_1, ndvi_0, weight) {
  
  initial_fit <- 100
  new_fit <- sum(abs(ndvi_1 - ndvi_0) * weight)
  ndvi_new <- ndvi_1

  while(initial_fit > new_fit) {
    initial_fit <- new_fit
    
    ndvi_new <- ifelse(ndvi_new >= ndvi_0, ndvi_new, ndvi_0) %>%
    sgolayfilt(p = 4, n = 21)
  
  new_fit <- sum(abs(ndvi_new - ndvi_0) * weight)
  }
  ndvi_new
}

# Iterate to get final time series

tpw_final <- tpw_weights %>%
  mutate(ndvi_new = iterative_fit(ndvi_filtered, ndvi_int, weight))

tpw_final %>%
  filter(plot_id %in% sample_plots) %>%
  pivot_longer(cols = c("ndvi_int", "ndvi_filtered", "ndvi_new"),
               names_to = "var", values_to = "value") %>%
  ggplot() +
  geom_line(aes(x = imgDate, y = value, colour = var)) +
  facet_wrap(~plot_id) +
  theme_bw() +
  scale_x_date(breaks = "6 months", minor_breaks = "3 month") +
  ylim(0, 1) +
  theme(legend.position="none",
        axis.text.x = element_text(angle = 60, vjust = 0.5, hjust=1))

tpw_final %>%
  filter(plot_id %in% sample_plots) %>%
  ggplot(aes(x = imgDate)) +
  geom_col(aes(y = precipitation / max(precipitation)), fill = "blue") +
  geom_line(aes(y = ndvi_new), colour = "grey", size = 1.5) +
  geom_point(aes(y = ndvi), colour = "red", size = 0.5) +
  geom_line(aes(y = ndvi), colour = "red") +
  facet_wrap(~plot_id) +
  theme_bw() +
  scale_x_date(breaks = "6 months", minor_breaks = "3 month") +
  ylim(0, 1) +
  theme(axis.text.x = element_text(angle = 60, vjust = 0.5, hjust=1))

```

Success! I've implemented a simple gap-filling smoother for the NDVI time series,
allowing me to quantify change over time at finer temporal resolutions. On visual
inspection it looks like it works well, creating a smooth time series of NDVI
values that hugs the top of the envelope of noisy original values. To get a
smoother result, I used slightly stronger smoothing than suggested in the paper
for the final smoother (setting p = 4 rather than 6), which reduced some of the
residual wiggle. However, I'm concerned it might be over-smoothing and thus
cutting off the start of the increase at the onset of rain. I'd be better with a
weaker smoother and then using other methods to fit the curve shape.

However, doing weaker smoothing is clearly causing overfitting. Single low NDVI
values pull the curve down, since the first step causes linear interpolation
between these values and adjacent ones, leading to a V-shaped initial time
series. Weak smoothing then keeps this V shape. I either need to go for stronger
smoothing, or somehow weight lone points as less important than areas with
several points.

I would still like to explore some other approaches to gap filling, either using
a different smoother (non-SG) or using spatial data. However, using spatial data
may by definition obscure the signal if I am looking to compare plots or areas. 

With a stronger smoothing, a clearer annual seasonal cycle becomes apparent.
The short rains (OND) lead to a surge in NDVI from the baseline, which dips a
little in Jan/Feb, before rising again in response to the long rains in MAM.
In some sites, these two seasons are not clearly differentiated, and the
vegetation is clearly still green from the former when the latter rains arrive.
However, the long dry season (June to September) sees a consistent decay in
greenness for all years with almost no rain. It would be the best period to
focus on for my analysis.

It would also, however, be clearly possible to extract other metrics, including
the delay between onset of rain and onset of green-up, the rate of green-up
(although this is very rapid in all cases), the height of the peak greenness
in relation to the antecedent rainfall and the overall rain use efficiency
(i.e., integrated NDVI vs. integrated rainfall per season). The novelty of my
study would be in (a) using Sentinel-2 data, i.e., much higher resolution than
done elsewhere, (b) making use of multiple metrics in tandem and (c) relating
these indices to real field measurements of percentage ground cover.

To extract decay rates, I need to first define the end points of the season. I
can use the season code I already created, or I can use NDVI decline

```{r plots}

plot_name <- "Alasukutan"

tpw_plot <- filter(tpw_final, plot_id == plot_name)

p1 <- tpw_plot %>%
  ggplot(aes(x = imgDate)) +
  geom_line(aes(y = ndvi), colour = "black") +
  geom_point(aes(y = ndvi), colour = "black", size = 0.4) +
  ylim(0, 1) +
  labs(x = "Date", y = "NDVI") +
  theme_clean()

p2 <- tpw_plot %>%
  ggplot(aes(x = imgDate)) +
  geom_line(aes(y = ndvi_int), colour = "blue") +
  geom_line(aes(y = ndvi), colour = "black") +
  geom_point(aes(y = ndvi), colour = "black", size = 0.4) +
  ylim(0, 1) +
  labs(x = "Date", y = "NDVI") +
  theme_clean()

p3 <- tpw_plot %>%
  ggplot(aes(x = imgDate)) +
  geom_line(aes(y = ndvi_int), colour = "black", alpha = 0.2) +
  geom_line(aes(y = ndvi), colour = "black", alpha = 0.2) +
  geom_point(aes(y = ndvi), colour = "black", size = 0.4, alpha = 0.2) +
  geom_line(aes(y = ndvi_filtered), colour = "brown", size = 1.2) +
  ylim(0, 1) +
  labs(x = "Date", y = "NDVI") +
  theme_clean()

p4 <- tpw_plot %>%
  ggplot(aes(x = imgDate)) +
  # geom_line(aes(y = ndvi_int), colour = "black", alpha = 0.2) +
  geom_line(aes(y = ndvi), colour = "black", alpha = 0.2) +
  geom_point(aes(y = ndvi), colour = "black", size = 0.4, alpha = 0.2) +
  # geom_line(aes(y = ndvi_filtered), colour = "black", size = 1, alpha = 0.2) +
  geom_line(aes(y = ndvi_new), colour = "purple", size = 1.2) +
  ylim(0, 1) +
  labs(x = "Date", y = "NDVI") +
  theme_clean()

p1; p2; p3; p4

ggsave(here("results", "figures", "ndvi_p1.jpg"), p1,
       width = 24, height = 16, units = "cm")
ggsave(here("results", "figures", "ndvi_p2.jpg"), p2,
       width = 24, height = 16, units = "cm")
ggsave(here("results", "figures", "ndvi_p3.jpg"), p3,
       width = 24, height = 16, units = "cm")
ggsave(here("results", "figures", "ndvi_p4_simple.jpg"), p4,
       width = 24, height = 16, units = "cm")

# Plot with NDVI envelope and rainfall
p5 <- ggplot(tpw_plot, aes(x = imgDate)) +
  geom_col(aes(y = precipitation / max(precipitation)), fill = "light blue") +
  geom_line(aes(y = ndvi_new), size = 1) +
  ylim(0, 1) +
  labs(x = "Date", y = "NDVI") +
  theme_clean()

ggsave(here("results", "figures", "ndvi_p5.jpg"), p5,
       width = 24, height = 16, units = "cm")

```

