---
title: "TPW Data Exploration"
output: html_notebook
author: "Guy Lomax"
date: 2023-03-13
---

```{r setup}

# Analysis
library(signal)
library(imputeTS)

# Data management
library(here)
library(tidyverse)
library(lubridate)
library(sf)

# Visualisation
library(ggthemes)

```

Load data.

Original data were in the form of a nested layer, with each point
(sampling event) accompanied by a nested table of 20 rows, one for every 5 m
along the 100 m N-S transect. Data were extracted as two separate files: a
shapefile containing 7,273 rows (one per sampling event) and a csv containing
130,102 rows (20 per sampling event or fewer in some cases).

```{r load}

tpw_points <- st_read(here("data", "raw", "vector", "tpw",
                           "Rangeland_Ufuatiliaji_wa_Nyanda_za_Malisho.shp"))
tpw_table <- read_csv(here("data", "raw", "csv", "tpw", "samplemetrics.csv"))
tz <- st_read(here("data", "raw", "vector", "natural_earth",
                   "ne_110m_admin_0_countries_fixed.shp")) %>%
  filter(NAME == "Tanzania") %>%
  select(NAME)

colnames(tpw_points)
colnames(tpw_table)

```
Join points to table, summarise and clean

```{r join_clean}

tpw_joined <- tpw_points %>%
  mutate(globalid = toupper(globalid)) %>%
  left_join(tpw_table, by = c("globalid" = "parentglobalid"))

# n_distinct(tpw_points$globalid)
# n_distinct(tpw_table$parentglobalid)

# n_measurements <- tpw_joined %>% group_by(globalid) %>% summarise(n = n())

# 1,255 out of 7,273 sampling events in tpw_points have zero rows from tpw_table
# associated with them. A further 53 rows have incorrect (not 20) images and
# may need cleaning.

# Columns to keep
vars <- c("globalid", "plot_id", "village", "village_pl", "dateTime",
          "plotburned", "plotgrazed", "plotcolor", "invasives_", "plotqualit",
          "sampleID", "markbare", "markbasal", "grassheight")

tpw_condensed <- tpw_joined %>%
  unite("plot_id", starts_with("plotID"),
        sep = "", na.rm = TRUE) %>%
  select(all_of(vars)) %>%
  group_by(globalid, plot_id, village, village_pl, dateTime) %>%
  summarise(n = n(),
            plotburned = first(plotburned),
            plotgrazed = first(plotgrazed),
            plotcolor = first(plotcolor),
            invasives = first(invasives_),
            plotquality = first(plotqualit),
            bare = sum(markbare),
            basal = sum(markbasal),
            grassheight = mean(grassheight)) %>%
  ungroup()

# Remove rows with no data or incorrect numbers of samples
# I could include the 8 rows with 21/22 samples by manually removing duplicates
# Also remove locations outside Tanzania

tpw_clean <- tpw_condensed %>%
  filter(n == 20) %>%
  filter(!is.na(plotburned) & !is.na(plotgrazed)) %>%
  filter(bare + basal == 100) %>%
  st_filter(tz)

tpw_locations <- tpw_clean %>%
  mutate(x = st_coordinates(geometry)[,1],
         y = st_coordinates(geometry)[,2]) %>%
  st_drop_geometry() %>%
  group_by(plot_id) %>%
  summarise(x = median(x),
            y = median(y)) %>%
  st_as_sf(crs = 4326, coords = c("x", "y"))

# How many are within Tanzania?
# st_filter(tpw_locations, tz) %>% nrow()
# All are within Tanzania - let's assume they're correct for now

tpw_locations_fixed <- tpw_clean %>%
  st_drop_geometry %>%
  left_join(tpw_locations)

# Write locations to shapefile for extraction

# st_write(tpw_locations,
#          here("data", "processed", "vector", "tpw", "tpw_locations.shp"),
#          delete_dsn = T)

```

Join with Sentinel 2 band and VI values and CHIRPS monthly precipitation
for each plot extracted from Google Earth Engine and explore time series.

```{r ts_eda}

tpw_bands <- read_csv(here("data", "processed", "csv", "tpw",
                           "tpwMonthlyValues.csv")) %>%
  select(-"system:index", -".geo") %>%
  rename(blue = B2,
         green = B3,
         red = B4,
         ir = B8,
         lc = Map) %>%
  filter(lc != 80) %>%  # Exclude plots flooded in 2021 (Lake Manyara shoreline)
  mutate(month = month(start),  # Extract month and year for joining
         year = year(start))

# Join with plot data

tpw_complete <- tpw_locations_fixed %>%
  mutate(month = month(dateTime),
         year = year(dateTime)) %>%
  inner_join(tpw_bands)

# Explore a few time series
# Select a sample of n random plots

set.seed(14941)
n <- 6
sample_plots <- unique(tpw_complete$plot_id) %>%
  sample(n)

cols_to_pivot <- c("bare", "basal", "grassheight", "msavi", "ndvi", "precipitation")

tpw_complete %>%
  filter(plot_id %in% sample_plots) %>%
  mutate(midpoint = as_date((as.numeric(start) + as.numeric(end)) / 2)) %>%
  pivot_longer(cols = all_of(cols_to_pivot),
               names_to = "var",
               values_to = "value") %>%
  ggplot(aes(x = midpoint, y = value, colour = plot_id)) +
  geom_line() +
  facet_wrap(~var, scales = "free") +
  theme_bw()

# Look at NDVI time series only
tpw_complete %>%
  filter(plot_id %in% sample_plots) %>%
  mutate(midpoint = as_date((as.numeric(start) + as.numeric(end)) / 2)) %>%
  ggplot(aes(x = midpoint, y = ndvi, colour = plot_id)) +
  geom_line() +
  theme_bw()


```

Explore twice-monthly time series:

```{r biweekly_ts}

tpw_biweekly <- read_csv(here("data", "processed", "csv", "tpw",
                              "tpwTSValues.csv")) %>%
  select(-"system:index", -".geo") %>%
  rename(blue = B2,
         green = B3,
         red = B4,
         ir = B8,
         lc = Map) %>%
  filter(lc != 80)

tpw_biweekly %>%
  group_by(plot_id) %>%
  summarise(frac_na = sum(is.na(ndvi) / n())) %>%
  ggplot(aes(x = frac_na)) +
  geom_histogram(bins = 20, fill = "green", colour = "dark green", alpha = 0.5)

tpw_complete %>%
  group_by(plot_id) %>%
  summarise(frac_na = sum(is.na(ndvi) / n())) %>%
  ggplot(aes(x = frac_na)) +
  geom_histogram(bins = 20, fill = "green", colour = "dark green", alpha = 0.5)

tpw_biweekly %>%
  filter(plot_id %in% sample_plots) %>%
  mutate(midpoint = as_date((as.numeric(start) + as.numeric(end)) / 2)) %>%
  ggplot(aes(x = midpoint, y = ndvi, colour = plot_id)) +
  geom_line() +
  theme_bw()

tpw_complete %>%
  filter(plot_id %in% sample_plots) %>%
  mutate(midpoint = as_date((as.numeric(start) + as.numeric(end)) / 2)) %>%
  ggplot(aes(x = midpoint, y = ndvi, colour = plot_id)) +
  geom_line() +
  theme_bw()

```

The two-week window dataset is much more useful than the monthly resolution,
but suffers from gaps caused by cloud cover. I would like to use this (or even
finer-resolution data) but would need to implement some gap-filling method.

Possible approaches:
A: Simple linear interpolation between adjacent dates
B: A more sophisticated gap-filling method, e.g., using local pixel rankings to
estimate increases based on both local pixels and dates, as in this paper:
https://ieeexplore.ieee.org/abstract/document/8276645
C: A temporal smoother, e.g., Savitsky-Golay or Gaussian Process
D: Use Sentinel-1 or MODIS as well as prior and subsequent NDVI values
to estimate an intermediate value.

Let's first look at data taking all S2 images (not aggregated monthly/biweekly):

```{r full_ts}

tpw_ts_all <- read_csv(here("data", "processed", "csv", "tpw",
                            "tpwTSValues_all.csv")) %>%
  select(-"system:index", -".geo") %>%
  rename(blue = B2,
         green = B3,
         red = B4,
         ir = B8,
         lc = Map) %>%
  filter(lc != 80)

# Merge double readings (more than one value in a day) by taking higher ndvi
# value or lower raw band value

# Import function s() from hablar package - manages NA values intuitively

s <- hablar::s

tpw_ts_unique <- tpw_ts_all %>%
  group_by(plot_id, imgDate) %>%
  summarise(blue = min(s(blue)), green = min(s(green)),
            red = min(s(red)), ir = min(s(ir)),
            ndvi = max(s(ndvi)), msavi = max(s(msavi)),
            lc = first(lc), precipitation = first(precipitation)) %>%
  ungroup()

tpw_ts_unique %>%
  group_by(plot_id) %>%
  summarise(frac_na = sum(is.na(ndvi) / n())) %>%
  ggplot(aes(x = frac_na)) +
  geom_histogram(bins = 20, fill = "green", colour = "dark green", alpha = 0.5)

tpw_ts_unique %>%
  filter(plot_id %in% sample_plots) %>%
  ggplot(aes(x = imgDate, y = ndvi, colour = plot_id)) +
  geom_line() +
  theme_bw()




```

Simple linear interpolation:

```{r lin_interp}

# Apply Chen et al. 2004 Savitzky-Golay-based gapfilling and smoothing algorithm
# to remove noise and cloud gaps

# First remove points with subsequent increase of > 0.1 in 5 days (assume cloud)
# Then linear interpolation for all gaps
tpw_all_int <- tpw_ts_unique %>%
  group_by(plot_id) %>%
  arrange(imgDate) %>%
  mutate(ndvi_spike = (lead(ndvi) - ndvi > 0.1),
         ndvi = na_if(ndvi, (ndvi_spike * ndvi)),
         ndvi_int = na_interpolation(ndvi)) %>%
  select(-ndvi_spike)
         

# Apply Savitzky-Golay filter to smooth signal
# For now, use m = 40-60 days (i.e., 8-12 images) and d = 3
# Will go back and code up internal optimisation of these values later

# sg_params <- tibble(p = rep(2:4, 7),
#                     m = rep(8:14, each = 3))
# 
# # Find optimal smoothing values for each time series
# find_least_error <- function(ts) {
#   # Calculate sum of squared errors for each combination of parameters
#   sse <- numeric(length = nrow(sg_params))
#   
#   for (i in seq_len(nrow(sg_params))) {
#     p <- sg_params$p[i]
#     m <- sg_params$m[i]
#     
#     new_ts <- sgolayfilt(ts, p = p, n = 2 * m + 1)
#     
#     sse[i] <- sum((ts - new_ts) ^ 2)
#   }
#   
#   # Choose iteration which has minimum sse
#   opt <- which.min(sse)
#   opt
# }

# tpw_sg1_opt <- tpw_all_int %>%
#   group_by(plot_id) %>%
#   mutate(opt = find_least_error(ndvi_int))

tpw_sg1 <- tpw_all_int %>%
  group_by(plot_id) %>%
  mutate(ndvi_filtered = sgolayfilt(ndvi_int, p = 3, n = 25))

# Assign weights to NDVI values in original (interpolated) series based on
# whether they fall above or below the trend curve for that TS.

tpw_weights <- tpw_sg1 %>%
  mutate(dist = ndvi_filtered - ndvi_int,
         max_dist = max(dist),
         weight = ifelse(dist > 0, 1 - (dist / max_dist), 1))

# Iterative approach to upper ndvi envelope
# Generate new time series by replacing "noisy" NDVI values with filtered ones
# Second, shorter-period SG filter

iterative_fit <- function(ndvi_1, ndvi_0, weight) {
  
  initial_fit <- 100
  new_fit <- sum(abs(ndvi_1 - ndvi_0) * weight)
  ndvi_new <- ndvi_1

  while(initial_fit > new_fit) {
    initial_fit <- new_fit
    
    ndvi_new <- ifelse(ndvi_new >= ndvi_0, ndvi_new, ndvi_0) %>%
    sgolayfilt(p = 4, n = 21)
  
  new_fit <- sum(abs(ndvi_new - ndvi_0) * weight)
  }
  ndvi_new
}

# Iterate to get final time series

tpw_final <- tpw_weights %>%
  mutate(ndvi_new = iterative_fit(ndvi_filtered, ndvi_int, weight))

tpw_final %>%
  filter(plot_id %in% sample_plots) %>%
  pivot_longer(cols = c("ndvi_int", "ndvi_filtered", "ndvi_new"),
               names_to = "var", values_to = "value") %>%
  ggplot() +
  geom_line(aes(x = imgDate, y = value, colour = var)) +
  facet_wrap(~plot_id) +
  theme_bw() +
  scale_x_date(breaks = "6 months", minor_breaks = "3 month") +
  ylim(0, 1) +
  theme(legend.position="none",
        axis.text.x = element_text(angle = 60, vjust = 0.5, hjust=1))

tpw_final %>%
  filter(plot_id %in% sample_plots) %>%
  ggplot(aes(x = imgDate)) +
  geom_col(aes(y = precipitation / max(precipitation)), fill = "blue") +
  geom_line(aes(y = ndvi_new), colour = "grey", size = 1.5) +
  geom_point(aes(y = ndvi), colour = "red", size = 0.5) +
  geom_line(aes(y = ndvi), colour = "red") +
  facet_wrap(~plot_id) +
  theme_bw() +
  scale_x_date(breaks = "6 months", minor_breaks = "3 month") +
  ylim(0, 1) +
  theme(axis.text.x = element_text(angle = 60, vjust = 0.5, hjust=1))

```

Success! I've implemented a simple gap-filling smoother for the NDVI time series,
allowing me to quantify change over time at finer temporal resolutions. On visual
inspection it looks like it works well, creating a smooth time series of NDVI
values that hugs the top of the envelope of noisy original values. To get a
smoother result, I used slightly stronger smoothing than suggested in the paper
for the final smoother (setting p = 4 rather than 6), which reduced some of the
residual wiggle. However, I'm concerned it might be over-smoothing and thus
cutting off the start of the increase at the onset of rain. I'd be better with a
weaker smoother and then using other methods to fit the curve shape.

However, doing weaker smoothing is clearly causing overfitting. Single low NDVI
values pull the curve down, since the first step causes linear interpolation
between these values and adjacent ones, leading to a V-shaped initial time
series. Weak smoothing then keeps this V shape. I either need to go for stronger
smoothing, or somehow weight lone points as less important than areas with
several points.

I would still like to explore some other approaches to gap filling, either using
a different smoother (non-SG) or using spatial data. However, using spatial data
may by definition obscure the signal if I am looking to compare plots or areas. 

With a stronger smoothing, a clearer annual seasonal cycle becomes apparent.
The short rains (OND) lead to a surge in NDVI from the baseline, which dips a
little in Jan/Feb, before rising again in response to the long rains in MAM.
In some sites, these two seasons are not clearly differentiated, and the
vegetation is clearly still green from the former when the latter rains arrive.
However, the long dry season (June to September) sees a consistent decay in
greenness for all years with almost no rain. It would be the best period to
focus on for my analysis.

It would also, however, be clearly possible to extract other metrics, including
the delay between onset of rain and onset of green-up, the rate of green-up
(although this is very rapid in all cases), the height of the peak greenness
in relation to the antecedent rainfall and the overall rain use efficiency
(i.e., integrated NDVI vs. integrated rainfall per season). The novelty of my
study would be in (a) using Sentinel-2 data, i.e., much higher resolution than
done elsewhere, (b) making use of multiple metrics in tandem and (c) relating
these indices to real field measurements of percentage ground cover.

To extract decay rates, I need to first define the end points of the season. I
can use the season code I already created, or I can use NDVI decline

```{r plots, eval = FALSE}

plot_name <- "Alasukutan"

tpw_plot <- filter(tpw_final, plot_id == plot_name)

p1 <- tpw_plot %>%
  ggplot(aes(x = imgDate)) +
  geom_line(aes(y = ndvi), colour = "black") +
  geom_point(aes(y = ndvi), colour = "black", size = 0.4) +
  ylim(0, 1) +
  labs(x = "Date", y = "NDVI") +
  theme_clean()

p2 <- tpw_plot %>%
  ggplot(aes(x = imgDate)) +
  geom_line(aes(y = ndvi_int), colour = "blue") +
  geom_line(aes(y = ndvi), colour = "black") +
  geom_point(aes(y = ndvi), colour = "black", size = 0.4) +
  ylim(0, 1) +
  labs(x = "Date", y = "NDVI") +
  theme_clean()

p3 <- tpw_plot %>%
  ggplot(aes(x = imgDate)) +
  geom_line(aes(y = ndvi_int), colour = "black", alpha = 0.2) +
  geom_line(aes(y = ndvi), colour = "black", alpha = 0.2) +
  geom_point(aes(y = ndvi), colour = "black", size = 0.4, alpha = 0.2) +
  geom_line(aes(y = ndvi_filtered), colour = "brown", size = 1.2) +
  ylim(0, 1) +
  labs(x = "Date", y = "NDVI") +
  theme_clean()

p4 <- tpw_plot %>%
  ggplot(aes(x = imgDate)) +
  # geom_line(aes(y = ndvi_int), colour = "black", alpha = 0.2) +
  geom_line(aes(y = ndvi), colour = "black", alpha = 0.2) +
  geom_point(aes(y = ndvi), colour = "black", size = 0.4, alpha = 0.2) +
  # geom_line(aes(y = ndvi_filtered), colour = "black", size = 1, alpha = 0.2) +
  geom_line(aes(y = ndvi_new), colour = "purple", size = 1.2) +
  ylim(0, 1) +
  labs(x = "Date", y = "NDVI") +
  theme_clean()

p1; p2; p3; p4

ggsave(here("results", "figures", "ndvi_p1.jpg"), p1,
       width = 24, height = 16, units = "cm")
ggsave(here("results", "figures", "ndvi_p2.jpg"), p2,
       width = 24, height = 16, units = "cm")
ggsave(here("results", "figures", "ndvi_p3.jpg"), p3,
       width = 24, height = 16, units = "cm")
ggsave(here("results", "figures", "ndvi_p4_simple.jpg"), p4,
       width = 24, height = 16, units = "cm")

# Plot with NDVI envelope and rainfall
p5 <- ggplot(tpw_plot, aes(x = imgDate)) +
  geom_col(aes(y = precipitation / max(precipitation)), fill = "light blue") +
  geom_line(aes(y = ndvi_new), size = 1) +
  ylim(0, 1) +
  labs(x = "Date", y = "NDVI") +
  theme_clean()

ggsave(here("results", "figures", "ndvi_p5.jpg"), p5,
       width = 24, height = 16, units = "cm")

```
Now to divide the time series into annual/seasonal cycles and fit an exponential
decay curve to each of them. Anticipated steps:
1. Filter to divide each time series into their hydrological year (Oct-Sep)
2. Remove or give NA values to seasons without a clearly defined NDVI curve
   (max - min < 0.2?)
2. Identify max and min NDVI values of the smoothed curve
3. Filter out data before or after those values
4. Transform NDVI by removing the min value, adding 1 and taking the log
5. Fitting a linear model to each one (if it seems appropriate!)

```{r exp_decay}

tpw_hyear <- tpw_final %>%
  mutate(hyear = year(imgDate + dmonths(4)))

head(tpw_hyear)

fit_decay <- function(df, col, log_mod = TRUE) {
  
  df <- df %>%
    select(all_of(col)) %>%
    rename(ndvi = 1)
  
  ndvi_ts = df$ndvi
  
  # Find max NDVI value
  max <- max(ndvi_ts, na.rm = T)
  max_index <- which.max(ndvi_ts)
  
  # Find min value subsequent to max value
  ndvi_after <- ndvi_ts[max_index:length(ndvi_ts)]
  min <- min(ndvi_after, na.rm = T)
  min_index <- which.min(ndvi_after) + max_index - 1
  
  # Test if max-min is sufficient to define meaningful decay curve
  if ((max - min) < 0.2) {
    NA
  } else {
    # Isolate max-min curve for fitting and scale to zero
    ndvi_curve <- ndvi_ts[max_index:min_index - 1] - min
    
    # Prepare and fit model
    log_ndvi <- log(ndvi_curve)
    day_curve <- seq_len(length(ndvi_curve))
    
    if (log_mod == TRUE) {
      mod <- lm(log_ndvi ~ day_curve)
    } else {
      mod <- lm(ndvi_curve ~ day_curve)
    }

    mod
  }
}

tpw_mod <- tpw_hyear %>%
  group_by(plot_id, hyear) %>%
  filter(hyear != 2023) %>%
  nest() %>%
  mutate(log_fit_new = map(data, fit_decay, col = "ndvi_new", log_mod = T),
         log_fit_old = map(data, fit_decay, col = "ndvi", log_mod = T),
         lin_fit_new = map(data, fit_decay, col = "ndvi_new", log_mod = F),
         lin_fit_old = map(data, fit_decay, col = "ndvi", log_mod = F))

# Pull out model performance metrics
# Define an augmented "glance()" function that can handle NAs

glance2 <- function(mod) {
  if(all(is.na(mod))) {
    NA
  } else {
    broom::glance(mod)
  }
}

tpw_performance <- tpw_mod %>%
  mutate(glance_log_n = map(log_fit_new, glance2),
         glance_log_o = map(log_fit_old, glance2),
         glance_lin_n = map(lin_fit_new, glance2),
         glance_lin_o = map(lin_fit_old, glance2)) %>%
  unnest(starts_with("glance"), names_sep = "_")

# r-squared comparison
tpw_performance %>%
  select(plot_id, hyear, ends_with("adj.r.squared")) %>%
  pivot_longer(cols = starts_with("glance"),
               names_to = "model", values_to = "r_squared") %>%
  ggplot(aes(x = r_squared, fill = model)) +
  geom_histogram(position = "identity", alpha = 0.4, bins = 60) +
  theme_bw()

mean(tpw_performance$glance_log_n_r.squared, na.rm = T)
mean(tpw_performance$glance_log_o_r.squared, na.rm = T)
mean(tpw_performance$glance_lin_n_r.squared, na.rm = T)
mean(tpw_performance$glance_lin_o_r.squared, na.rm = T)
  
```

Damn! The highest r-squared is for the linear fit with the smoothed data.
I think this might be because I'm not picking up the decay leg of the curve
very effectively. I will try a new algorithm for doing this:
1. Take the first and second derivatives of the smoothed NDVI curve
2. Group by plot_id and year, and identify all maxima and minima in a list for
each.
3. Identify the period with the greatest increase (min to max) and
greatest decrease (max to min) in the time series
4. Calculate the rate of increase (linear?) and decay (exponential) for each one.
Maybe I can write a function that chooses the linear or exponential fit for each
case study based on r-squared.

```{r max_min}

tpw_derivatives <- tpw_hyear %>%
  select(plot_id, hyear, imgDate, ndvi, ndvi_new) %>%
  group_by(plot_id) %>%
  arrange(imgDate) %>%
  mutate(ndvi_smooth = smooth.spline(imgDate, ndvi_new)$y) %>%
  mutate(ndvi_1d = (lag(ndvi_new) - lead(ndvi_new)) / as.numeric(lag(imgDate) - lead(imgDate)),
         ndvi_1ds = (lag(ndvi_smooth) - lead(ndvi_smooth)) / as.numeric(lag(imgDate) - lead(imgDate)),
         ndvi_2d = (lag(ndvi_1d) - lead(ndvi_1d)) / as.numeric(lag(imgDate) - lead(imgDate)),
         ndvi_2ds = (lag(ndvi_1ds) - lead(ndvi_1ds)) / as.numeric(lag(imgDate) - lead(imgDate)))

tpw_derivatives %>%
  filter(plot_id == "Alasukutan") %>%
  pivot_longer(starts_with("ndvi"), names_to = "var") %>%
  ggplot(aes(x = imgDate, y = value)) +
  geom_line() +
  facet_wrap(~var, scales = "free") +
  theme_bw()

# Using the data smoothed with cubic splines seems to significantly reduce
# noise in the first and second derivatives.

# Identify points where 1st derivative == 0

tpw_max_min <- tpw_derivatives %>%
  mutate(is_max = (ndvi_1ds * lag(ndvi_1ds) < 0) & (ndvi_2ds < 0),
         is_min = (ndvi_1ds * lag(ndvi_1ds) < 0) & (ndvi_2ds > 0))

tpw_max_min %>%
  filter(plot_id == "ALamunyani") %>%
  ggplot(aes(x = imgDate)) +
  geom_line(aes(y = ndvi_smooth), colour = "blue", alpha = 0.5) +
  geom_line(aes(y = (ndvi_new - min(ndvi_new)))) +
  # geom_point(aes(y = as.numeric(is_max)), colour = "blue", size = 1.5) +
  # geom_point(aes(y = as.numeric(is_min)), colour = "red", size = 1.5) +
  theme_bw() +
  ylim(0, 1)

# Successfully identifying maxima and minima; saddle points shouldn't occur

# For each hydrological year, identify the greatest increase and decrease
# between subsequent maxima/minima

# 1. Create data frame of only max and min rows (including NDVI, imgDate and
#    plot_id) arranged by imgDate
# 2. Calculate NDVI differences between rows
# 3. Extract imgDate for min value (largest drop =, i.e., decay) and max value
#    (largest increase, i.e., recovery)

tpw_events <- tpw_max_min %>%
  filter(is_max | is_min) %>%
  select(plot_id, imgDate, hyear, ndvi_new, ndvi_smooth, is_max, is_min) %>%
  mutate(ndvi_diff = lead(ndvi_new) - ndvi_new,
         ndvi_smooth_diff = lead(ndvi_smooth) - ndvi_smooth,
         next_imgDate = lead(imgDate)) %>%
  group_by(plot_id, hyear) %>%
  summarise(growth_start = imgDate[which.max(ndvi_diff)],
            growth_end = next_imgDate[which.max(ndvi_diff)],
            decay_start = imgDate[which.min(ndvi_diff)],
            decay_end = next_imgDate[which.min(ndvi_diff)])

head(tpw_events)

# Join with original time series for fitting
tpw_fitting <- tpw_hyear %>%
  left_join(tpw_events) %>%
  group_by(plot_id) %>%
  mutate(ndvi_scaled = ndvi_new - min(ndvi_new)) %>%
  group_by(plot_id, hyear) %>%
  mutate(growth_leg = (imgDate >= growth_start & imgDate <= growth_end),
         decay_leg = (imgDate >= decay_start & imgDate <= decay_end))

growth_fit <- function(ts, date) {
  df <- tibble(date, ts) %>%
    mutate(date = as.numeric(date),
           date_scaled = (date - min(date)) / (max(date) - min(date)))
  
  logisticModel <- tryCatch(
    nls(ts~K/(1+exp(-r*(date_scaled - x0))), data=df, 
        start=list(K=max(df$ts), r=5, x0 = 0.5), 
        control=list(maxiter=2000, minFactor=.00000000001)),
    error=function(err) NA
  )
  
  logisticModel
}

decay_fit <- function(ts, date) {
  df <- tibble(date, ts) %>%
    mutate(date = as.numeric(date),
           date_scaled = (date - min(date)) / (max(date) - min(date)))
  
  logisticModel <- tryCatch(
    nls(ts~K/(1+exp(-r*(date_scaled - x0))), data=df, 
        start=list(K=max(df$ts), r=-5, x0 = 0.5), 
        control=list(maxiter=2000, minFactor=.00000000001)),
    error=function(err) NA
  )
  
  logisticModel
}

tpw_growth_model <- tpw_fitting %>%
  filter(!(hyear %in% c(2019,2023))) %>%
  filter(growth_leg) %>%
  summarise(mod = list(growth_fit(ndvi_scaled, imgDate)))

tpw_decay_model <- tpw_fitting %>%
  filter(!(hyear %in% c(2019,2023))) %>%
  filter(decay_leg) %>%
  summarise(mod = list(decay_fit(ndvi_scaled, imgDate)))

# Test overall fit
tidy <- function(mod) {
  if (is.logical(mod)) {
    NA
  } else {
    broom::tidy(mod)
  }
}
growth_params <- tpw_growth_model %>%
  mutate(params = map(mod, tidy)) %>%
  unnest(params) %>%
  mutate(pc_error = std.error / estimate * 100)

growth_performance <- tpw_growth_model %>%
  mutate(glance = tryCatch(map(mod, broom::glance), error = function(err) NA)) %>%
  unnest(glance)

summary(growth_fitted$mod[[1]])

```

We're almost there. The main problems are:
a) At the start and end of the plot (where we have incomplete years), it ignores
the true upward or downward legs and picks up intermediate wiggles. This loses
data and gives false results (e.g., in 2019 for Alasukutan). For now, I'm
excluding incomplete years (2019 and 2023), but that's a lot of data to throw
away. Maybe if I use the Sentinel-2 L1C data...
b) It still treats as a single decay curve any stretch with inflection points that
don't reach local maxima/minima, but still throw off the shape. Ideally, I
would identify changes in curvature from convex-up to concave-up, or the stretch
with greatest local slope... But that may be too awkward and fudgy. For now,
I've just gone ahead with model fitting and then generated an NA value if the
model does not converge. I should also convert models with very poor fit or
poorly constrained parameters to NA.

The decay curves seem less well fitted than the growth curves, interestingly.
Perhaps they are less of a good fit to the logistic curve as they tend to 
decline steeply and then slow down rather than having a symmetrical shape, and
also may have additional humps or slower decay rates in.

Next steps:
- Plot a few fitted models of both types to see how they're doing
- Extract key parameters from models (height of curve, r etc.)
- Check average error for a few different approaches (using global vs. local
  min NDVI to scale, filtering out curves with very small increases/decreases,
  logistic vs. exponential)

```{r mod_vis}

augment <- function(mod) {
  if (is.logical(mod)) {
    NA
  } else {
    broom::augment(mod)
  }
}

growth_fitted <- tpw_growth_model %>%
  mutate(values = map(mod, augment)) %>%
  unnest(values) %>%
  arrange(plot_id, hyear)

tpw_growth_fitted <- tpw_fitting %>%
  ungroup() %>%
  arrange(plot_id, hyear) %>%
  left_join(growth_fitted, by = c("plot_id", "hyear", "ndvi_scaled" = "ts"))

head(tpw_growth_fitted)

tpw_growth_fitted %>%
  filter(plot_id == "ALamunyani") %>%
  ggplot(aes(x = imgDate)) +
  geom_line(aes(y = ndvi_scaled)) +
  geom_line(aes(y = .fitted), colour = "orange", size = 1.5) +
  # facet_wrap(~plot_id) +
  theme_bw() +
  ylim(0, 1)

# Decay
decay_fitted <- tpw_decay_model %>%
  mutate(values = map(mod, augment)) %>%
  unnest(values) %>%
  arrange(plot_id, hyear)

tpw_decay_fitted <- tpw_fitting %>%
  ungroup() %>%
  arrange(plot_id, hyear) %>%
  left_join(decay_fitted, by = c("plot_id", "hyear", "ndvi_scaled" = "ts"))

head(tpw_decay_fitted)

tpw_decay_fitted %>%
  filter(plot_id == "ALamunyani") %>%
  ggplot(aes(x = imgDate)) +
  geom_line(aes(y = ndvi_scaled)) +
  geom_line(aes(y = .fitted), colour = "orange", size = 1.5) +
  # facet_wrap(~plot_id) +
  theme_bw() +
  ylim(0, 1)
                                                   
growth_params %>%
  filter(term == "K") %>%
  ggplot(aes(x = estimate)) +
  geom_histogram(fill = "light green", colour = "green", alpha = 0.6,
                 binwidth = 0.05)


```

