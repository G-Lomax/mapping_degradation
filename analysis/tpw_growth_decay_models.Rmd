---
title: "TPW Growth and Decay Models"
output: html_notebook
author: "Guy Lomax"
date: 2023-03-24
---

This notebook imports Sentinel-2 band and vegetation index time series data
from Google Earth Engine for the Tanzania People and Wildlife plots, cleans the
time series data by smoothing and gapfilling, then fits a series of simple
exponential, logistic and linear models to the growth phase and decay phase of
the NDVI curve in each season.


```{r setup}

# Analysis
library(signal)
library(imputeTS)

# Data management
library(here)
library(tidyverse)
library(lubridate)
library(sf)

# Visualisation
library(ggthemes)

```

# Load data

Data are in the form of a csv downloaded from Google Earth Engine containing
Sentinel-2 surface reflectances for the RGBIR bands, as well as NDVI and MSAVI
estimates, for every image in the L2A collection that intersects each point.

Clouds, cloud shadows and other problem pixels have already been removed using
the L2A product's SCA pixel classification flags, but clouds, shadows and
other atmospheric noise may remain.


```{r load_data}

tpw_ts_all <- read_csv(here("data", "processed", "csv", "tpw",
                            "tpwTSValues_all.csv")) %>%
  select(-"system:index", -".geo") %>%
  rename(blue = B2,
         green = B3,
         red = B4,
         ir = B8,
         lc = Map) %>%
  filter(lc != 80)  # Remove pixels sometimes within lake boundaries

# Merge double readings (if more than one value in a day) by taking higher ndvi
# value or lower raw band value (likely to be least cloud-contaminated)

# Import function s() from hablar package - manages NA values intuitively

s <- hablar::s

tpw_ts_unique <- tpw_ts_all %>%
  group_by(plot_id, imgDate) %>%
  summarise(blue = min(s(blue)), green = min(s(green)),
            red = min(s(red)), ir = min(s(ir)),
            ndvi = max(s(ndvi)), msavi = max(s(msavi)),
            lc = first(lc), precipitation = first(precipitation)) %>%
  ungroup()

tpw_ts_unique %>%
  filter(plot_id == "Alasukutan") %>%
  ggplot(aes(x = imgDate, y = ndvi)) +
  geom_line() +
  theme_bw() +
  ylim(0, 1)


```

Create smooth, continuous time series by filling NA values using intelligent
gapfilling algorithm from Chen et al. (2004) based on Savitzky-Golay filtering.
The method also boosts values identified as likely cloud-contaminated that have
not been removed by the Sentinel-2 quality assessment algorithm.

```{r gapfilling}

# First remove points with subsequent increase of > 0.1 in 5 days (assume cloud)
# Then linear interpolation for all gaps
tpw_all_int <- tpw_ts_unique %>%
  group_by(plot_id) %>%
  arrange(imgDate) %>%
  mutate(ndvi_spike = (lead(ndvi) - ndvi > 0.1),
         ndvi = na_if(ndvi, (ndvi_spike * ndvi)),
         ndvi_int = na_interpolation(ndvi)) %>%
  select(-ndvi_spike)
         

# Apply Savitzky-Golay filter to smooth signal
# For now, use m = 60 days (12 images either side) and d = 3
# Allowing the smoother to optimise internally (as recommended in Chen et al.
# 2004) tends to lead to overfitting as there is no penalty on wiggliness.

# sg_params <- tibble(p = rep(2:4, 7),
#                     m = rep(8:14, each = 3))
# 
# # Find optimal smoothing values for each time series
# find_least_error <- function(ts) {
#   # Calculate sum of squared errors for each combination of parameters
#   sse <- numeric(length = nrow(sg_params))
#   
#   for (i in seq_len(nrow(sg_params))) {
#     p <- sg_params$p[i]
#     m <- sg_params$m[i]
#     
#     new_ts <- sgolayfilt(ts, p = p, n = 2 * m + 1)
#     
#     sse[i] <- sum((ts - new_ts) ^ 2)
#   }
#   
#   # Choose iteration which has minimum sse
#   opt <- which.min(sse)
#   opt
# }

# tpw_sg1_opt <- tpw_all_int %>%
#   group_by(plot_id) %>%
#   mutate(opt = find_least_error(ndvi_int))

tpw_sg1 <- tpw_all_int %>%
  group_by(plot_id) %>%
  mutate(ndvi_filtered = sgolayfilt(ndvi_int, p = 3, n = 25))

# Assign weights to NDVI values in original (interpolated) series based on
# whether they fall above or below the trend curve for that TS.

tpw_weights <- tpw_sg1 %>%
  mutate(dist = ndvi_filtered - ndvi_int,
         max_dist = max(dist),
         weight = ifelse(dist > 0, 1 - (dist / max_dist), 1))

# Iterative approach to upper ndvi envelope
# Generate new time series by replacing "noisy" NDVI values with filtered ones
# Second, shorter-period SG filter

iterative_fit <- function(ndvi_1, ndvi_0, weight) {
  
  initial_fit <- 100
  new_fit <- sum(abs(ndvi_1 - ndvi_0) * weight)
  ndvi_new <- ndvi_1

  while(initial_fit > new_fit) {
    initial_fit <- new_fit
    
    ndvi_new <- ifelse(ndvi_new >= ndvi_0, ndvi_new, ndvi_0) %>%
    sgolayfilt(p = 4, n = 21)
  
  new_fit <- sum(abs(ndvi_new - ndvi_0) * weight)
  }
  ndvi_new
}

# Iterate to get final time series

tpw_final <- tpw_weights %>%
  mutate(ndvi_new = iterative_fit(ndvi_filtered, ndvi_int, weight))

tpw_final %>%
  filter(plot_id == "Alasukutan") %>%
  pivot_longer(cols = c("ndvi_int", "ndvi_filtered", "ndvi_new"),
               names_to = "var", values_to = "value") %>%
  ggplot() +
  geom_line(aes(x = imgDate, y = value, colour = var)) +
  theme_bw() +
  scale_x_date(breaks = "6 months", minor_breaks = "3 month") +
  ylim(0, 1) +
  theme(legend.position="none",
        axis.text.x = element_text(angle = 60, vjust = 0.5, hjust=1))

```
The gapfilling method seems to work reasonably well, although with some
remaining flaws:
1. Individual low values amid cloud cover sometimes pull the whole curve down
when I am not sure that is warranted.
2. The polynomial fit introduces some additional wiggles (e.g., at the end of
the dry season) and also cuts off some of the initial increase.
3. It is still not completely smooth.

I would still like to explore some other approaches to gap filling, either using
a different smoother (non-SG) or using spatial data. However, using spatial data
may by definition obscure the signal if I am looking to compare plots or areas. 

Now to divide the time series into annual/seasonal cycles and fit an exponential
decay curve to each of them. Anticipated steps:
1. Filter to divide each time series into their hydrological year (Sep-Aug)
2. Remove or give NA values to seasons without a clearly defined NDVI curve
(max - min < 0.2?)
3. Identify all maxima and minima of NDVI curves
4. For each hydrological year, identify the min-max leg (growth) and the max-min
leg (decay) with the largest difference.
5. Fit logarithmic, logistic and linear models to each.


```{r max_min}

# Define hydrological year as Sep-Aug
tpw_hyear <- tpw_final %>%
  mutate(hyear = year(imgDate + dmonths(4)))

# Fit smooth spline to NDVI curve to reduce noise in derivatives
# Calculate 1st and 2nd derivatives to identify maxima and minima
tpw_derivatives <- tpw_hyear %>%
  select(plot_id, hyear, imgDate, ndvi, ndvi_new) %>%
  group_by(plot_id) %>%
  arrange(imgDate) %>%
  mutate(ndvi_smooth = smooth.spline(imgDate, ndvi_new)$y) %>%
  mutate(ndvi_1d = (lag(ndvi_smooth) - lead(ndvi_smooth)) / as.numeric(lag(imgDate) - lead(imgDate)),
         ndvi_2d = (lag(ndvi_1d) - lead(ndvi_1d)) / as.numeric(lag(imgDate) - lead(imgDate)))

tpw_derivatives %>%
  filter(plot_id == "Alasukutan") %>%
  pivot_longer(starts_with("ndvi"), names_to = "var") %>%
  ggplot(aes(x = imgDate, y = value)) +
  geom_line() +
  facet_wrap(~var, scales = "free") +
  theme_bw()

# Using the data smoothed with cubic splines seems to significantly reduce
# noise in the first and second derivatives.

# Identify points where 1st derivative == 0

tpw_max_min <- tpw_derivatives %>%
  mutate(is_max = (ndvi_1d * lag(ndvi_1d) < 0) & (ndvi_2d < 0),
         is_min = (ndvi_1d * lag(ndvi_1d) < 0) & (ndvi_2d > 0))

tpw_max_min %>%
  filter(plot_id == "Alasukutan") %>%
  ggplot(aes(x = imgDate)) +
  geom_line(aes(y = ndvi_smooth), colour = "blue", alpha = 0.5) +
  geom_line(aes(y = (ndvi_new - min(ndvi_new)))) +
  geom_point(aes(y = as.numeric(is_max)), colour = "blue", size = 1.5) +
  geom_point(aes(y = as.numeric(is_min)), colour = "red", size = 1.5) +
  theme_bw() +
  ylim(0, 1)

# Successfully identifying maxima and minima

# For each hydrological year, identify the greatest increase and decrease
# between subsequent maxima/minima

# 1. Create data frame of only max and min rows (including NDVI, imgDate and
#    plot_id) arranged by imgDate
# 2. Calculate NDVI differences between rows
# 3. Extract imgDate for min value (largest drop, i.e., decay) and max value
#    (largest increase, i.e., recovery)
# NB: To avoid ensure 

tpw_events <- tpw_max_min %>%
  filter(is_max | is_min) %>%
  select(plot_id, imgDate, hyear, ndvi_new, ndvi_smooth, is_max, is_min) %>%
  mutate(ndvi_diff = lead(ndvi_new) - ndvi_new,
         ndvi_smooth_diff = lead(ndvi_smooth) - ndvi_smooth,
         prev_imgDate = lag(imgDate),
         next_imgDate = lead(imgDate)) %>%
  group_by(plot_id, hyear) %>%
  summarise(growth_start = imgDate[which.max(ndvi_diff)],
            growth_end = next_imgDate[which.max(ndvi_diff)],
            decay_start = imgDate[which.min(ndvi_diff)],
            decay_end = next_imgDate[which.min(ndvi_diff)])

# Join with original time series for fitting
tpw_fitting <- tpw_hyear %>%
  left_join(tpw_events) %>%
  group_by(plot_id) %>%
  mutate(ndvi_scaled = ndvi_new - min(ndvi_new)) %>%
  group_by(plot_id, hyear) %>%
  mutate(growth_leg = (imgDate >= growth_start & imgDate <= growth_end),
         decay_leg = (imgDate >= decay_start & imgDate <= decay_end))

growth_fit <- function(ts, date) {
  df <- tibble(date, ts) %>%
    mutate(date = as.numeric(date),
           date_scaled = (date - min(date)) / (max(date) - min(date)))
  
  logisticModel <- tryCatch(
    nls(ts~K/(1+exp(-r*(date_scaled - x0))), data=df, 
        start=list(K=max(df$ts), r=5, x0 = 0.5), 
        control=list(maxiter=2000, minFactor=.00000000001)),
    error=function(err) NA
  )
  
  logisticModel
}

decay_fit <- function(ts, date) {
  df <- tibble(date, ts) %>%
    mutate(date = as.numeric(date),
           date_scaled = (date - min(date)) / (max(date) - min(date)))
  
  logisticModel <- tryCatch(
    nls(ts~K/(1+exp(-r*(date_scaled - x0))), data=df, 
        start=list(K=max(df$ts), r=-5, x0 = 0.5), 
        control=list(maxiter=2000, minFactor=.00000000001)),
    error=function(err) NA
  )
  
  logisticModel
}

tpw_growth_model <- tpw_fitting %>%
  filter(!(hyear %in% c(2019,2023))) %>%
  filter(growth_leg) %>%
  summarise(mod = list(growth_fit(ndvi_scaled, imgDate)))

tpw_decay_model <- tpw_fitting %>%
  filter(!(hyear %in% c(2019,2023))) %>%
  filter(decay_leg) %>%
  summarise(mod = list(decay_fit(ndvi_scaled, imgDate)))

# Test overall fit
tidy <- function(mod) {
  if (is.logical(mod)) {
    NA
  } else {
    broom::tidy(mod)
  }
}
growth_params <- tpw_growth_model %>%
  mutate(params = map(mod, tidy)) %>%
  unnest(params) %>%
  mutate(pc_error = std.error / estimate * 100)

growth_performance <- tpw_growth_model %>%
  mutate(glance = tryCatch(map(mod, broom::glance), error = function(err) NA)) %>%
  unnest(glance)

summary(growth_fitted$mod[[1]])

```

We're almost there. The main problems are:
a) At the start and end of the plot (where we have incomplete years), it ignores
the true upward or downward legs and picks up intermediate wiggles. This loses
data and gives false results (e.g., in 2019 for Alasukutan). For now, I'm
excluding incomplete years (2019 and 2023), but that's a lot of data to throw
away. Maybe if I use the Sentinel-2 L1C data...
b) It still treats as a single decay curve any stretch with inflection points that
don't reach local maxima/minima, but still throw off the shape. Ideally, I
would identify changes in curvature from convex-up to concave-up, or the stretch
with greatest local slope... But that may be too awkward and fudgy. For now,
I've just gone ahead with model fitting and then generated an NA value if the
model does not converge. I should also convert models with very poor fit or
poorly constrained parameters to NA.

The decay curves seem less well fitted than the growth curves, interestingly.
Perhaps they are less of a good fit to the logistic curve as they tend to 
decline steeply and then slow down rather than having a symmetrical shape, and
also may have additional humps or slower decay rates in.

Next steps:
- Plot a few fitted models of both types to see how they're doing
- Extract key parameters from models (height of curve, r etc.)
- Check average error for a few different approaches (using global vs. local
  min NDVI to scale, filtering out curves with very small increases/decreases,
  logistic vs. exponential)

```{r mod_vis}

augment <- function(mod) {
  if (is.logical(mod)) {
    NA
  } else {
    broom::augment(mod)
  }
}

growth_fitted <- tpw_growth_model %>%
  mutate(values = map(mod, augment)) %>%
  unnest(values) %>%
  arrange(plot_id, hyear)

tpw_growth_fitted <- tpw_fitting %>%
  ungroup() %>%
  arrange(plot_id, hyear) %>%
  left_join(growth_fitted, by = c("plot_id", "hyear", "ndvi_scaled" = "ts"))

head(tpw_growth_fitted)

tpw_growth_fitted %>%
  filter(plot_id == "ALamunyani") %>%
  ggplot(aes(x = imgDate)) +
  geom_line(aes(y = ndvi_scaled)) +
  geom_line(aes(y = .fitted), colour = "orange", size = 1.5) +
  # facet_wrap(~plot_id) +
  theme_bw() +
  ylim(0, 1)

# Decay
decay_fitted <- tpw_decay_model %>%
  mutate(values = map(mod, augment)) %>%
  unnest(values) %>%
  arrange(plot_id, hyear)

tpw_decay_fitted <- tpw_fitting %>%
  ungroup() %>%
  arrange(plot_id, hyear) %>%
  left_join(decay_fitted, by = c("plot_id", "hyear", "ndvi_scaled" = "ts"))

head(tpw_decay_fitted)

tpw_decay_fitted %>%
  filter(plot_id == "ALamunyani") %>%
  ggplot(aes(x = imgDate)) +
  geom_line(aes(y = ndvi_scaled)) +
  geom_line(aes(y = .fitted), colour = "orange", size = 1.5) +
  # facet_wrap(~plot_id) +
  theme_bw() +
  ylim(0, 1)
                                                   
growth_params %>%
  filter(term == "K") %>%
  ggplot(aes(x = estimate)) +
  geom_histogram(fill = "light green", colour = "green", alpha = 0.6,
                 binwidth = 0.05)


```

