---
title: "Local GPP Scaling"
output: html_notebook
date: 2024-01-09
author: "Guy Lomax"
editor_options: 
  chunk_output_type: console
---

This notebook implements a clustering algorithm using input geospatial layers
to define similar "land capability classes" as defined by Prince et al. (XXXX)
and developed by Noojipady et al. (XXXX) and Li et al. (XXXX). These references
typically use remotely sensed NPP products or use NDVI as a proxy, and hence
refer to the method as "local NPP scaling" (LNS). Using GPP data, we thus
apply "local GPP scaling" (LGS) as an alternative.


``` {r setup}

# Data handling
library(tidyverse)
library(sf)
library(terra)
library(here)
library(data.table)

# Analysis
library(mlr3verse)
library(tictoc)
library(future)

# Visualisation
library(tmap)

# nc <- 19
# plan(multisession, workers = nc)

```


We define land capability classes (LCCs) based on the multi-annual mean values of
precipitation, air temperature, photosynthetically active radiation and potential ET,
as well as % tree cover, soil sand fraction and slope. This is to prevent the instability
in LCCs that results if new classes are derived for each year in the dataset.

We conduct two analyses, using the following variables:
- Mean annual precipitation, temperature, sand fraction, tree cover, slope, PAR and PET
- The above variables plus mean values of precipitation intensity and timing variables

```{r load}

# Country boundaries
ke_tz <- st_read(here("data", "raw", "vector", "kenya_tanzania.geojson"))

# Covariate layers
# Static
static_covariates <- rast(here("data", "raw", "raster", "covariateMaps",
                                 "staticVars.tif"))

twi <- rast(here("data", "processed", "raster", "hydroSHEDS",
                 "hydrosheds_twi_fd8.tif"))

names(twi) <- "twi"

# Load dynamic covariates as a named list of rasters (one per year)
dynamic_covariates_paths <- Sys.glob(here("data", "raw", "raster", "covariateMaps",
                                          "dynamicVars*.tif"))

years <- str_extract(dynamic_covariates_paths, "\\d\\d\\d\\d")
dynamic_covariates <- map(dynamic_covariates_paths, rast)
names(dynamic_covariates) <- years

# Load precipitation covariates as a named list of rasters (one per year)
ppt_covariate_paths <- Sys.glob(here("data", "raw", "raster", "covariateMaps",
                                      "pptVars*.tif"))
ppt_covariates <- map(ppt_covariate_paths, rast)
names(ppt_covariates) <- years

# Crop static and ppt variables to dynamic variable extent

ppt_covariates_crop <- map(ppt_covariates, crop, dynamic_covariates[[1]])
static_covariates_crop <- crop(static_covariates, dynamic_covariates[[1]])

```

``` {r process}

dynamic_vars <- c("GPP", "precipitation", "tMean", "parMean", "potentialET")
ppt_vars <- c("intensity", "ugi", "meanPptDay_anomaly")
static_vars <- c("sand", "slope", "wriTCFrac")

mean_ppt <- map(dynamic_covariates, function(r) r$precipitation) %>% 
  rast() %>%
  mean()

mean_ppt_day <- map(ppt_covariates_crop, function(r) r$meanPptDay) %>% 
  rast() %>%
  mean()

# Map over years to generate annual layers with all data and convert to df

vars_annual <- map(years, function(y) {
  
  message("Extracting variables for year ", y)
  dynamic_year <- dynamic_covariates[[y]]
  ppt_year <- ppt_covariates_crop[[y]]
  
  ppt_anomaly <- dynamic_year$precipitation / mean_ppt
  mean_ppt_day_anomaly <- ppt_year$meanPptDay - mean_ppt_day
  
  names(ppt_anomaly) <- "ppt_anomaly"
  names(mean_ppt_day_anomaly) <- "meanPptDay_anomaly"
  
  dynamic_year_all <- c(dynamic_year, ppt_year, ppt_anomaly, mean_ppt_day_anomaly)
  
  dynamic_year_subset <- subset(dynamic_year_all, c(dynamic_vars, ppt_vars))
  
  static_subset <- subset(static_covariates_crop, static_vars)
  
  c(dynamic_year_subset, static_subset)
})

names(vars_annual) <- years


# Convert to data.frame
combined_vars_dfs <- map(years, function(y) {
  year_raster <- vars_annual[[y]]
  message("Converting to data.frame: ", y)
  
  df <- as.data.frame(year_raster, cell = TRUE, xy = TRUE, na.rm = TRUE)
  
  data.table::as.data.table(df)
})

names(combined_vars_dfs) <- years

write_rds(combined_vars_dfs,
          here("data", "processed", "rds", "lgs", "combined_dfs.rds"))

```


We apply k-means clustering algorithm using only environmental covariates (i.e.,
not including spatial coordinates) to derive LCCs. For k-means clustering, the
value of k (the desired number of clusters) must be manually specified by the
user. We test a range of k values from 10 to 100. A higher value of k increases
the granularity of the analysis, allowing smaller subsets of covariate space
to be mapped and reducing heterogeneity in clusters. However, higher values of
k also reduce the number of pixels within each cluster, reducing the precision
of the quantile method and making it less likely that undegraed


``` {r clustering, eval = FALSE}

# Function to implement k-means with specified k
fit_k_means <- function(data, k) {
  
  task <- data %>%
    select(-x, -y, -cell, -GPP) %>%
    as_task_clust()
  
  message("Finding clusters for k = ", k)
  
  # Set up learners
  lrn_km <- lrn("clust.kmeans",
                predict_type = "partition",
                centers = k,
                algorithm = "Lloyd",
                nstart = 10,
                iter.max = 1000
  )
  
  # Preprocessing pipelines
  
  po_scale <- po("scale")
  
  ppl_km <- as_learner(po_scale %>>% lrn_km)
  
  # Perform clustering and add clusters to original data
  
  predictions_km <- ppl_km$train(task)$predict(task)
  
  message("Clustering complete")
  
  predictions_km$partition
}

set.seed(999)

tic()
annual_clusters_k100 <- map(combined_vars_dfs, fit_k_means)
toc()

write_rds(annual_clusters_k100, here("data", "processed", "rds", "clusters_annual.rds"))

```


Once points are assigned to clusters, we can extract the 90th percentile of GPP
for each cluster as a proxy for a potential or reference GPP. The LGS scaled
GPP value can then be calculated as either the ratio between estimated and
potential GPP (possibly the difference between estimated and minimum GPP) or as
the simple difference between estimated and potential. Here, we use the ratio
in order to avoid biasing the result to areas with a larger range of GPP values
and for consistency with the RPI method.

```{r lgs}

annual_clusters_k100 <- read_rds(here("data", "processed", "rds", "clusters_annual.rds"))

sample_points_clustered <- map(years, function(y) {
  df <- combined_vars_dfs[[y]]
  clusters <- annual_clusters_k100[[y]]
  
  df$cluster <- clusters
  df$year <- as.numeric(y)
  
  df
}) %>% bind_rows()

# 90th percentile for each cluster
quantiles <- sample_points_clustered %>%
  group_by(cluster) %>%
  summarise(potential = quantile(GPP, 0.9),
            mean = mean(GPP)) %>%
  ungroup()

sample_points_quantiles <- left_join(sample_points_clustered, quantiles)

lgs <- sample_points_quantiles %>%
  group_by(cluster, k) %>%
  mutate(lgs_abs = GPP - potential,
         lgs_ratio = GPP / potential,
         lgs_scaled = (GPP - min(GPP)) / (potential - min(GPP))) %>%
  ungroup()

hist(lgs$lgs_ratio, breaks = 100)
hist(lgs$lgs_scaled, breaks = 100)

# Convert back to raster

k_list <- c(10, 20, 50, 100)

lgs_rasters <- list(length = length(k_list))

for (i in seq_along(k_list)) {
  message("Processing for k = ", k_list[i])
  lgs_k <- filter(lgs, k == k_list[i])
  
  lgs_rast <- lgs_k %>%
    select(x, y, cluster, GPP, potential, lgs_abs, lgs_ratio, lgs_scaled) %>%
    rast(crs = crs(combined_rast), extent = ext(combined_rast))
  
  lgs_rasters[[i]] <- lgs_rast
}

```

Now to visualise LGS results on a map:

```{r lgs_viz}

medians <- lgs %>%
  group_by(k) %>%
  summarise(median_abs = median(lgs_abs),
            median_ratio = median(lgs_ratio),
            median_scaled = median(lgs_scaled),
            median_potential = median(potential))

# Map results
map_k <- 100
index <- which(k_list == map_k)

lgs_map <- tm_shape(ke_tz) +
  tm_fill("grey90") +
  tm_shape(lgs_rasters[[index]]$lgs_ratio) +
  tm_raster(col.scale = tm_scale_continuous(
    limits = c(0.4,1.2),
    midpoint = 0.8,
    outliers.trunc = c(T,T),
    values = "RdBu"
  ),
  col.legend = tm_legend(reverse = TRUE)
  ) +
  tm_shape(ke_tz) +
  tm_borders()

gpp_map <- tm_shape(ke_tz) +
  tm_fill("grey90") +
  tm_shape(lgs_rasters[[index]]$GPP) +
  tm_raster(col.scale = tm_scale_continuous(
    limits = c(0, 4000),
    midpoint = 2000,
    outliers.trunc = c(T,T),
    values = "RdBu"),
    col.legend = tm_legend(reverse = TRUE)
    ) +
  tm_shape(ke_tz) +
  tm_borders()

lgs_potential_map <- tm_shape(ke_tz) +
  tm_fill("grey90") +
  tm_shape(lgs_rasters[[index]]$potential) +
  tm_raster(col.scale = tm_scale_continuous(
    limits = c(0, 4000),
    midpoint = 2000,
    outliers.trunc = c(T,T),
    values = "RdBu"),
    col.legend = tm_legend(reverse = TRUE)
    ) +
  tm_shape(ke_tz) +
  tm_borders()

lgs_map
gpp_map
lgs_potential_map


# tm_shape(ke_tz) +
#   tm_fill("grey90") +
#   tm_shape(lgs_rasters[[3]]$cluster) +
#   tm_raster(col.scale = tm_scale_categorical(
#     values = "viridis"),
#     col.legend = tm_legend(show = FALSE)
#     ) +
#   tm_shape(ke_tz) +
#   tm_borders()

```

The potential GPP as estimated from the cluster analysis varies widely, but
certain areas (e.g., Eastern Kenya) have large areas with the same estimated
potential GPP. This gives rise to blocky artifacts and discontinuities in the
LGS map at the cluster boundaries that are unlikely to reflect reality.

In addition, the results are sensitive to the number of clusters chosen. With
small k, clusters are very large and result in very blocky maps of potential
GPP and LGS ratios. For larger k, this problem is reduced, but the range of
LGS ratios becomes smaller and the method becomes less sensitive.

``` {r save}

# Save LGS rasters to disk for comparison with other results

for (i in seq_along(k_list)) {
  filename <- paste0("lgs_rast_with_ppt_", k_list[i], ".tif")
  
  r <- lgs_rasters[[i]]
  
  writeRaster(r, here("data", "processed", "raster", "lgs", filename),
              overwrite = TRUE)
}


```

