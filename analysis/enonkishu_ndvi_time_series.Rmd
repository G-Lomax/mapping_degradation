---
title: "Enonkishu Conservancy NDVI time series example"
author: "Guy Lomax"
date: '2023-03-31'
output:
  html_document:
    df_print: paged
---

## Generating high-resolution NDVI time series

Sentinel-2 provides global satellite imagery at 5-day intervals and at
10-meter resolution, making it one of the most precise freely available
satellite datasets currently available. Sentinel-2 imagery at 5-day
intervals is available for East Africa from early 2017, providing about
six years of continuous data.

The fine resolution of Sentinel-2 allows us to understand vegetation
patterns and change on smaller scales than previously possible, to more
easily distinguish between open areas and tree- or shrub-covered areas,
and to connect remote sensing data to field data collected at the level
of individual sample plots or transects.

Here, I have used the cloud-based platform Google Earth Engine to
extract raw Sentinel-2 data from September 2017 to September 2022 for
a handful of grassland points in and around Enonkishu, and converted the raw
data into a continuous NDVI time series. This time series can then be used to
understand productivity, recovery rates and resilience of different areas, and
could ultimately be embedded in a Google Earth Engine app that can map these
changes across the landscape.

```{r setup, include = F}

# Analysis
library(signal)
library(imputeTS)

# Data management
library(here)
library(tidyverse)
library(lubridate)
library(sf)

# Visualisation
library(ggthemes)
library(tmap)

tmap_mode("view")

knitr::opts_chunk$set(
  comment = "#>",
  echo = FALSE, warning = FALSE, message = FALSE,
  fig.keep = "all", fig.width = 8
)

```

I have selected six sample points in and around Enonkishu Conservancy to
demonstrate the time series analysis. For each point, I have extracted the
NDVI (Normalised Difference Vegetation Index) values for all Sentinel-2 images
of that location from 2017-2022 at 10m resolution.

The location of the points and the raw time series data are shown below:


```{r full_ts, cache = T}

ts_all <- read_csv(here("data", "processed", "csv", "enonkishu",
                        "laleenokTSValues_all_l1c.csv")) %>%
  mutate(across(.cols = c("B2", "B3", "B4", "B8", "ndvi", "msavi"),
                .fns = ~na_if(.x, -100))) %>%
  mutate(across(.cols = c("B2", "B3", "B4", "B8", "ndvi", "msavi"),
                .fns = function(x) (ifelse(maskFrac > 0.8, NA, x)))) %>%
  mutate(across(.cols = c("B2", "B3", "B4", "B8", "ndvi", "msavi"),
                .fns = function(x) (x + 100 * maskFrac) / (1 - maskFrac))) %>%
  mutate(id = (substr(`system:index`, 1, 1) %>% as.numeric()) + 1,
         id = LETTERS[id]) %>%
  st_as_sf(coords = c("x", "y"), crs = 4326) %>%
  select(-`system:index`, -.geo) %>%
  rename(blue = B2,
         green = B3,
         red = B4,
         ir = B8,
         lc = Map)

# Merge double readings (more than one value in a day) by taking higher ndvi
# value or lower raw band value

# Import function s() from hablar package - manages NA values intuitively

s <- hablar::s

ts_unique <- ts_all %>%
  group_by(id, imgDate) %>%
  summarise(blue = min(s(blue)), green = min(s(green)),
            red = min(s(red)), ir = min(s(ir)),
            ndvi = max(s(ndvi)), msavi = max(s(msavi)),
            lc = first(lc), precipitation = first(precipitation)) %>%
  ungroup()

# Plot points

ts_points <- ts_unique %>%
  group_by(id) %>%
  slice(1)

tm_shape(ts_points) + tm_dots(labels = "id", col = "yellow") +
  tm_text(text = "id", xmod = 2, ymod = 2, col = "white") +
  tm_view(set.view = 14) + tm_basemap(leaflet::providers$Esri.WorldImagery)

ts_unique %>%
  ggplot(aes(x = imgDate, y = ndvi)) +
  facet_wrap(~id) +
  # geom_line() +
  geom_point(size = 0.5) +
  theme_clean() +
  ylim(0, 0.8) +
  labs(x = "Date", y = "NDVI")


```

The raw data is quite noisy and has some missing values due to cloud cover, but
we can clean this up by using an intelligent smoothing method that reconstructs
the original, smooth values.

```{r gapfilling, cache = T, dependson= "full_ts"}

# Apply Chen et al. 2004 Savitzky-Golay-based gapfilling and smoothing algorithm
# to remove noise and cloud gaps

# Conduct a linear interpolation to create a complete time series
# Then remove points with prior decrease or subsequent increase of > 0.1 in
# 5 days (assume cloud)
# Then fill these gaps with another linear interpolation

all_int <- ts_unique %>%
  group_by(id) %>%
  arrange(imgDate) %>%
  mutate(ndvi_int = na_interpolation(ndvi),
         ndvi_spike = (lead(ndvi_int) - ndvi_int > 0.05),
         ndvi_drop = (lag(ndvi_int) - ndvi_int > 0.05),
         ndvi_int = na_if(ndvi_int, ((ndvi_spike | ndvi_drop) * ndvi)),
         ndvi_int = na_interpolation(ndvi_int)) %>%
  select(-ndvi_spike, -ndvi_drop)
         

# Apply Savitzky-Golay filter to smooth signal
# For now, use m = 40-60 days (i.e., 8-12 images) and d = 3
# Will go back and code up internal optimisation of these values later

# sg_params <- tibble(p = rep(2:4, 7),
#                     m = rep(8:14, each = 3))
# 
# # Find optimal smoothing values for each time series
# find_least_error <- function(ts) {
#   # Calculate sum of squared errors for each combination of parameters
#   sse <- numeric(length = nrow(sg_params))
#   
#   for (i in seq_len(nrow(sg_params))) {
#     p <- sg_params$p[i]
#     m <- sg_params$m[i]
#     
#     new_ts <- sgolayfilt(ts, p = p, n = 2 * m + 1)
#     
#     sse[i] <- sum((ts - new_ts) ^ 2)
#   }
#   
#   # Choose iteration which has minimum sse
#   opt <- which.min(sse)
#   opt
# }

# sg1_opt <- all_int %>%
#   group_by(id) %>%
#   mutate(opt = find_least_error(ndvi_int))

sg1 <- all_int %>%
  group_by(id) %>%
  mutate(ndvi_filtered = sgolayfilt(ndvi_int, p = 2, n = 25))

# Assign weights to NDVI values in original (interpolated) series based on
# whether they fall above or below the trend curve for that TS.

weights <- sg1 %>%
  mutate(dist = ndvi_filtered - ndvi_int,
         max_dist = max(dist),
         weight = ifelse(dist > 0, 1 - (dist / max_dist), 1))

# Iterative approach to upper ndvi envelope
# Generate new time series by replacing "noisy" NDVI values with filtered ones
# Second, shorter-period SG filter

iterative_fit <- function(ndvi_1, ndvi_0, weight) {
  
  initial_fit <- 100
  new_fit <- sum(abs(ndvi_1 - ndvi_0) * weight)
  ndvi_new <- ndvi_1

  while(initial_fit > new_fit) {
    initial_fit <- new_fit
    
    ndvi_new <- ifelse(ndvi_new >= ndvi_0, ndvi_new, ndvi_0) %>%
    sgolayfilt(p = 3, n = 21)
  
  new_fit <- sum(abs(ndvi_new - ndvi_0) * weight)
  }
  ndvi_new
}

# Iterate to get final time series

final <- weights %>%
  mutate(ndvi_new = iterative_fit(ndvi_filtered, ndvi_int, weight)) %>%
  select(-dist, -max_dist, -weight, -ndvi_int, -ndvi_filtered)

# final %>%
#   filter(id %in% sample_plots) %>%
#   pivot_longer(cols = c("ndvi_int", "ndvi_filtered", "ndvi_new"),
#                names_to = "var", values_to = "value") %>%
#   ggplot() +
#   geom_line(aes(x = imgDate, y = value, colour = var)) +
#   facet_wrap(~id) +
#   theme_bw() +
#   scale_x_date(breaks = "6 months", minor_breaks = "3 month") +
#   ylim(0, 1) +
#   theme(legend.position="none",
#         axis.text.x = element_text(angle = 60, vjust = 0.5, hjust=1))

smooth_ndvi <- final %>%
  filter(id == "B") %>%
  ggplot(aes(x = imgDate)) +
  # geom_col(aes(y = precipitation / max(precipitation)), fill = "blue") +
  geom_point(aes(y = ndvi), colour = "white", size = 1) +
  geom_line(aes(y = ndvi_new), colour = "white", size = 1.5) +
  # geom_line(aes(y = ndvi), colour = "grey40") +
  # facet_wrap(~plot_id) +
  theme_clean() +
  scale_x_date(breaks = "1 year", date_labels = "%Y") +
  ylim(0, 1) +
  theme(rect = element_rect(fill = "transparent", linewidth = 0)) +
  theme(axis.text.x = element_text(colour = "transparent", size = 24),
        axis.text.y = element_text(colour = "white", size = 24),
        axis.title.x = element_text(colour = "transparent", size = 28),
        axis.title.y = element_text(colour = "white", size = 28),
        axis.line.x = element_line(colour = "transparent", linewidth = 2),
        axis.line.y = element_line(colour = "white", linewidth = 2),
        axis.ticks.x = element_line(colour = "transparent", linewidth = 2),
        axis.ticks.y = element_line(colour = "transparent", linewidth = 2),
        panel.grid.major.y = element_line(colour = "white", linewidth = 1),
        panel.grid.major.x = element_line(colour = "white", linewidth = 0.5, linetype = "dotted")) +
  labs(x = "Date", y = "NDVI")

smooth_ndvi

ggsave(here("results", "figures", "smooth_ndvi.png"),
       smooth_ndvi,
       width = 24, height = 12, units = "cm", dpi = 600,
       bg = "transparent")





```

The grey shows the original data, while the green shows the smoothed
version. You can now clearly see the annual cycle, and the differences
between sites. For example, you can see that site F (which I think is in the
unmanaged/unrestricted grazing area) has lower average NDVI and also seems to
have a bigger/faster drop from the peaks to the valleys.

We can use these curves to extract some useful measures, such as productivity,
rain use efficiency and recovery rate.

## Extracting NDVI recovery and decay rates from satellite data

Two things we might be interested in are how rapidly vegetation grows at the
start of the rainy season, and how quickly it stops growing and senesces at the
end of the season. We can estimate this from the data by fitting simple
statistical models to the start and end of the seasonal curves. The blue is for
models fitted to the growth phase and the brown for the browning/decay phase.
Sometimes the model doesn't fit properly or fits the wrong part of the curve,
but generally it works quite well.


```{r metrics, cache = T, dependson = c("full_ts", "gapfilling")}

hyear <- final %>%
  mutate(hyear = year(imgDate + dmonths(4))) %>%
  filter(hyear > 2016) %>%
  st_drop_geometry()

metrics <- hyear %>%
  group_by(id) %>%
  mutate(min_ndvi = min(ndvi_new),
         ndvi_pc10 = quantile(ndvi_new, 0.1),
         growing_season = ndvi_new > ndvi_pc10) %>%
  group_by(id, hyear) %>%
  summarise(min_ndvi = first(min_ndvi),
            seasonal_min_ndvi = min(ndvi_new),
            peak_ndvi = max(ndvi_new) - min_ndvi,
            ndvi_auc = sum(ndvi_new * growing_season) - 
              (sum(growing_season) * min_ndvi),
            season_ppt = sum(precipitation * growing_season))

ppt_plot <- hyear %>%
  ggplot(aes(x = imgDate)) +
  # geom_line(aes(y = ndvi_new), colour = "green") +
  # geom_point(aes(y = ndvi), colour = "grey", size = 0.2) +
  geom_col(aes(y = precipitation), colour = "white") +
  scale_x_date(breaks = "1 year", date_labels = "%Y") +
  # scale_y_continuous(position = "right") +
  # facet_wrap(~id) +
  # ylim(0, 1) +
  theme_clean() +
  theme(rect = element_rect(fill = "transparent", linewidth = 0)) +
  theme(axis.text.x = element_text(colour = "white", size = 24),
        axis.text.y = element_text(colour = "white", size = 24),
        axis.title.x = element_text(colour = "white", size = 28),
        axis.title.y = element_text(colour = "white", size = 28),
        axis.line.x = element_line(colour = "white", linewidth = 2),
        axis.line.y = element_line(colour = "white", linewidth = 2),
        axis.ticks.x = element_line(colour = "transparent", linewidth = 2),
        axis.ticks.y = element_line(colour = "transparent", linewidth = 2),
        panel.grid.major.y = element_line(colour = "white", linewidth = 1),
        panel.grid.major.x = element_line(colour = "white", linewidth = 0.5, linetype = "dotted")) +
  labs(x = "Date", y = "Precipitation")

# metrics %>%
#   ggplot(aes(x = season_ppt, y = ndvi_auc, colour = as.factor(id), shape = as.factor(id))) +
#   geom_point() +
#   # geom_smooth(method = "lm", se = FALSE, show.legend = FALSE) +
#   # geom_line(size = 0.2) +
#   theme_bw() +
#   labs(x = "Seasonal rainfall (mm)",
#        y = "Integrated seasonal NDVI",
#        colour = "Plot",
#        shape = "Plot",
#        title = "Relationship between NDVI and rainfall",
#        subtitle = "Slope of line is the rain use efficiency of the plot")


ggsave(here("results", "figures", "rainfall_example.png"),
       ppt_plot,
       width = 24, height = 12, units = "cm", dpi = 600,
       bg = "transparent")

combined_plot <- ggpubr::ggarrange(smooth_ndvi, ppt_plot,
                                   nrow = 2, ncol = 1, align = "v")

ggsave(here("results", "figures", "combined_example.png"),
       combined_plot,
       width = 24, height = 24, units = "cm", dpi = 600,
       bg = "transparent")

```


```{r growth_decay_models, cache = T,dependson = c("full_ts", "gapfilling", "metrics")}

# Take first and second derivatives to identify maxima and minima in time series
# First smooth with cubic splines to reduce noise in derivatives
derivatives <- hyear %>%
  select(id, hyear, imgDate, ndvi, ndvi_new) %>%
  group_by(id) %>%
  arrange(imgDate) %>%
  mutate(ndvi_smooth = smooth.spline(imgDate, ndvi_new)$y) %>%
  mutate(ndvi_1d = (lag(ndvi_new) - lead(ndvi_new)) / as.numeric(lag(imgDate) - lead(imgDate)),
         ndvi_1ds = (lag(ndvi_smooth) - lead(ndvi_smooth)) / as.numeric(lag(imgDate) - lead(imgDate)),
         ndvi_2d = (lag(ndvi_1d) - lead(ndvi_1d)) / as.numeric(lag(imgDate) - lead(imgDate)),
         ndvi_2ds = (lag(ndvi_1ds) - lead(ndvi_1ds)) / as.numeric(lag(imgDate) - lead(imgDate)))

# Identify points where 1st derivative == 0 as maxima (2nd derivative < 0) or
# minima (2nd derivative > 0)
# Since 1st derivative won't be exactly zero in discrete time series, look for
# where 1st derivative undergoes a change in sign

max_min <- derivatives %>%
  mutate(is_max = (ndvi_1ds * lag(ndvi_1ds) < 0) & (ndvi_2ds < 0),
         is_min = (ndvi_1ds * lag(ndvi_1ds) < 0) & (ndvi_2ds > 0))

# For each hydrological year, identify the greatest increase and decrease
# between subsequent maxima/minima

# 1. Create data frame of only max and min rows (including NDVI, imgDate and
#    id) arranged by imgDate
# 2. Calculate NDVI differences between rows
# 3. Extract imgDate for min value (largest drop =, i.e., decay) and max value
#    (largest increase, i.e., recovery)
#
# To ensure segments are correctly identified within an hyear, we first find the
# maxima (end of growth segment and start of decay) and then find the minima
# before and after, respectively

events <- max_min %>%
  filter(is_max | is_min) %>%
  select(id, imgDate, hyear, ndvi_new, ndvi_smooth, is_max, is_min) %>%
  mutate(lead_diff = lead(ndvi_new) - ndvi_new,
         lag_diff = lag(ndvi_new) - ndvi_new,
         next_imgDate = lead(imgDate),
         prev_imgDate = lag(imgDate)) %>%
  group_by(id, hyear) %>%
  summarise(growth_start = prev_imgDate[which.min(lag_diff)],
            growth_end = imgDate[which.min(lag_diff)],
            decay_start = imgDate[which.min(lead_diff)],
            decay_end = next_imgDate[which.min(lead_diff)])

# Join with original time series for fitting
fitting <- hyear %>%
  left_join(events) %>%
  group_by(id, hyear) %>%
  mutate(ndvi_scaled = ndvi_new - min(ndvi_new)) %>%
  mutate(growth_leg = (imgDate >= growth_start & imgDate <= growth_end),
         decay_leg = (imgDate >= decay_start & imgDate <= decay_end))

# Define functions to fit growth and decay legs

growth_fit <- function(ts, date, model = "linear") {
  df <- tibble(date, ts) %>%
    mutate(date = as.numeric(date),
           date_scaled = (date - min(date)) / (max(date) - min(date)))
  
  if(model == "linear") {
    
    mod <- tryCatch(
      lm(ts ~ date_scaled, data = df),
      error = function(err) NA
    )
    
  } else if(model == "logistic") {
    
    mod <- tryCatch(
      nls(ts ~ max(df$ts) / (1 + exp(-r * (date_scaled - x0))), data = df,
          start = list(r = 5, x0 = 0.5),
          control = list(maxiter = 2000, minFactor = .00000000001)),
      error = function(err) NA
    )
    
  } else if(model == "exponential") {
    
    mod <- tryCatch(
      nls(ts ~ a * exp(r * date_scaled), data = df,
          start = list(a = 0.05, r = 2),
          control = list(maxiter = 2000, minFactor = .00000000001)),
      error = function(err) NA
    )
  } else {
    
    stop("Invalid model name. Choose one of 'linear', 'logistic' or 'exponential'")
    
  }
}

decay_fit <- function(ts, date, model = "linear") {
  df <- tibble(date, ts) %>%
    mutate(date = as.numeric(date),
           date_scaled = (date - min(date)) / (max(date) - min(date)))
  
  if(model == "linear") {
    
    mod <- tryCatch(
      lm(ts ~ date_scaled, data = df),
      error = function(err) NA
    )
    
  } else if(model == "logistic") {
    
    mod <- tryCatch(
      nls(ts ~ max(df$ts) / (1 + exp(-r * (date_scaled - x0))), data = df,
          start = list(r = -5, x0 = 0.5),
          control = list(maxiter = 2000, minFactor = .00000000001)),
      error = function(err) NA
    )
    
  } else if(model == "exponential") {
    
    mod <- tryCatch(
      nls(ts ~ a * exp(r * date_scaled), data = df,
          start = list(a = max(ts), r = -2),
          control = list(maxiter = 2000, minFactor = .00000000001)),
      error = function(err) NA
    )
  } else {
    
    stop("Invalid model name. Choose one of 'linear', 'logistic' or 'exponential'")
    
  }
}

# Apply functions to each site and hydrological year
t1 <- Sys.time()
growth_model <- fitting %>%
  filter(growth_leg) %>%
  summarise(mod = list(growth_fit(ndvi_scaled, imgDate, "logistic")))
t2 <- Sys.time()
cat("Fitting time - growth: ", t2 - t1)

t1 <- Sys.time()
decay_model <- fitting %>%
  filter(decay_leg) %>%
  summarise(mod = list(decay_fit(ndvi_scaled, imgDate, "logistic")))
t2 <- Sys.time()
cat("Fitting time - decay: ", t2 - t1)

# Test overall fit
tidy <- function(mod) {
  if (is.logical(mod)) {
    NA
  } else {
    broom::tidy(mod)
  }
}
growth_params <- growth_model %>%
  mutate(params = map(mod, tidy)) %>%
  unnest(params) %>%
  mutate(pc_error = std.error / estimate * 100)

growth_performance <- growth_model %>%
  mutate(glance = tryCatch(map(mod, broom::glance), error = function(err) NA)) %>%
  unnest(glance)

decay_params <- decay_model %>%
  mutate(params = map(mod, tidy)) %>%
  unnest(params) %>%
  mutate(pc_error = std.error / estimate * 100)

decay_performance <- decay_model %>%
  mutate(glance = tryCatch(map(mod, broom::glance), error = function(err) NA)) %>%
  unnest(glance)

# mean(growth_performance$sigma)
# median(growth_performance$sigma)
# mean(decay_performance$sigma)
# median(decay_performance$sigma)
# 
# sum(is.na(growth_model$mod))
# sum(is.na(decay_model$mod))

```

```{r model_vis}

# Extract model fitted values
augment <- function(mod) {
  if (is.logical(mod)) {
    NA
  } else {
    broom::augment(mod)
  }
}

growth_fitted <- growth_model %>%
  mutate(values = map(mod, augment)) %>%
  unnest(values) %>%
  select(-mod, -date_scaled) %>%
  rename(fitted_growth = .fitted,
         resids_growth = .resid)

decay_fitted <- decay_model %>%
  mutate(values = map(mod, augment)) %>%
  unnest(values) %>%
  select(-mod, -date_scaled) %>%
  rename(fitted_decay = .fitted,
         resids_decay = .resid)

# Combine growth and decay fitted values with original data frame

fitted <- fitting %>%
  ungroup() %>%
  full_join(growth_fitted, by = c("id", "hyear", "ndvi_scaled" = "ts")) %>%
  full_join(decay_fitted, by = c("id", "hyear", "ndvi_scaled" = "ts")) %>%
  select(-starts_with("growth"), -starts_with("decay")) %>%
  rename(growth = fitted_growth, decay = fitted_decay) %>%
  pivot_longer(cols = c(growth, decay),
               names_to = "leg", values_to = "fitted") %>%
  group_by(id, hyear) %>%
  mutate(fitted = fitted + min(ndvi_new, na.rm = TRUE))

# Visualise
fitted %>%
  ggplot(aes(x = imgDate)) +
  geom_line(aes(y = ndvi_new)) +
  geom_line(aes(y = fitted, colour = leg), size = 1.5, alpha = 0.8) +
  scale_colour_brewer(palette = "Dark2", direction = -1) +
  facet_wrap(~id) +
  theme_clean() +
  ylim(-0.1, 1) +
  labs(x = "Date", y = "NDVI", colour = "Phase")

# Visualise growth/decay rate and growth/decay rate divided by ceiling

all_params <- bind_rows("growth" = growth_params, "decay" = decay_params,
                        .id = "leg") %>%
  pivot_wider(id_cols = c("id", "hyear", "leg"),
              names_from = term, values_from = estimate)

# all_params %>%
#   filter((leg == "growth" & r > 0) | (leg == "decay" & r < 0)) %>%
#   ggplot(aes(x = r, fill = leg)) +
#   geom_histogram(colour = "black", alpha = 0.6, binwidth = 0.5, position = "identity") +
#   scale_fill_brewer(palette = "Dark2", direction = -1) +
#   geom_vline(xintercept = 0) +
#   theme_clean() +
#   labs(x = "Growth or decay rate", y = "Count")

# all_params %>%
#   filter((leg == "growth" & r > 0) | (leg == "decay" & r < 0)) %>%
#   mutate(r = abs(r)) %>%
#   ggplot(aes(x = as.factor(id), y = r, colour = leg)) +
#   geom_boxplot() +
#   theme_clean()

all_params %>%
  group_by(id, leg) %>%
  summarise(r = abs(mean(r))) %>%
  pivot_wider(id_cols = id, names_from = leg, values_from = r) %>%
  rename("Point ID" = id,
         "Average decay rate" = decay,
         "Average growth rate" = growth) %>%
  mutate(across(everything(), ~signif(.x, 3))) %>%
  knitr::kable()


```

From here, we could use these data in a few different ways:

1.  We could compare some of these metrics between different grazing blocks or
    look at how boma placement is affecting the vegetation.
2.  We could examine the drivers behind the differences in growth or
    decay rates, for example distinguishing the effects of rainfall,
    soil type or management.
3.  We could compare these metrics between regions with similar areas
    outside Enonkishu to understand or demonstrate impact.
4.  We could compare these metrics to field vegetation monitoring data to see if
    it is able to capture other metrics of interest on the ground.
    