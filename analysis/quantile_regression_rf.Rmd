---
title: "Quantile Regression of Rangeland Productivity - Quantile Regression Forests"
output: html_notebook
author: Guy Lomax
date: 2023-06-19
editor_options: 
  markdown: 
    wrap: 72
  chunk_output_type: console
---

This Notebook conducts quantile regression on rangeland productivity
data using Quantile Regression Forests (QRF), modelling annual gross
primary productivity (GPP) as a function of environmental variables

```{r setup, include = FALSE}

# Data handling
library(tidyverse)
library(sf)
library(here)

# Analysis
library(mlr3verse)
library(mlr3spatiotempcv)
library(ranger)
library(future)
library(tictoc)

# Visualisation
library(tmap)

# # Parallelisation
nc <- availableCores() / 4
plan(multisession, workers = nc)

```

```{r load, include = FALSE}

# Country boundaries
ke_tz <- st_read(here("data", "raw", "vector", "kenya_tanzania.geojson"))

# Sample point data
sample_points_raw <- st_read(here("data", "raw", "vector", "coarseScaleSample.geojson"))

twi <- read_csv(here("data", "processed", "csv", "twi_sample.csv"))

sample_points_ppt <- st_read(here("data", "raw", "vector", "pptDistributionSample.geojson"))

# Precipitation distribution metrics

ppt_distribution <- st_read(here("data", "raw", "vector", "pptDistributionSample.geojson")) %>%
    dplyr::select(-c("id", "allRangelands", "grassland", "igbp", "shrubland", "trees", "geometry")) %>%
    st_drop_geometry() %>%
    pivot_longer(cols = !index, names_to = "colname", values_to = "value") %>%
    separate_wider_delim(colname, "_", names = c("variable", "year")) %>%
    mutate(year = as.numeric(year)) %>%
    pivot_wider(names_from = "variable", values_from = "value")


```

```{r clean, include = FALSE}

yearly_vars <- c("GPP", "precipitation", "tMean", "parMean", "potentialET",
                 "meanPptDay", "intensity", "ugi")

sample_points <- sample_points_raw %>%
  mutate(x = st_coordinates(geometry)[,1],
         y = st_coordinates(geometry)[,2]) %>%
  bind_cols(twi) %>%
  rename(twi = hydrosheds_twi_fd8) %>%
  select(-allRangelands, -igbp, -shrubland, -grassland, -trees, -ID, -id) %>%
  pivot_longer(cols = starts_with(yearly_vars),
               names_to = "var", values_to = "value") %>%
  separate_wider_delim(var, "_", names = c("data", "year")) %>%
  pivot_wider(names_from = data, values_from = value) %>%
  mutate(year = as.numeric(year)) %>%
  left_join(ppt_distribution, by = c("index", "year")) %>%
  filter(GPP > 0) %>%
  st_as_sf(coords = c("x", "y"), crs = 4326)

# Calculate derived and normalised variables
# Convert landform to factor variable
sample_points_derived <- sample_points %>%
  group_by(index) %>%
  mutate(mean_ppt = mean(precipitation),
         ppt_anomaly = (precipitation - mean_ppt) / mean_ppt * 100,
         mean_meanPptDay = mean(meanPptDay),
         meanPptDay_anomaly = meanPptDay - mean_meanPptDay,
         landform = factor(landform)) %>%
  ungroup()


```

# Data preparation for RF modelling

We select variables to use for modelling, remove rows containing NA
values (the TWI raster has some pixels with NA values) and convert to a
spatiotemporal regression task. We assign point id ("id") as our index
of spatial location and "year" as our index of time.

We then choose our learner (random forest regression through the ranger
package) with quantile regression enabled, and define two alternative
resampling (cross- validation) strategies: random CV and
"Leave-location-and-time-out" CV that ensures the validation set for
each fold does not overlap in either time or space with the training
set.

When defining the learner, we initially choose crude, plausible
hyperparameters of mtry.ratio = 0.5 and sample.fraction = 0.5. However,
we set the initial minimum node size to 100 (around 0.05% of the dataset)
to mitigate overfitting and to allow sufficient terminal node size that
quantiles can be calculated. For our purposes, absolute predictive
accuracy is less important than retaining enough variability in nodes to
allow assessment of quantiles.

```{r data_prep}

# Create task

vars_to_retain <- c(
  "index",
  "year",
  "GPP",
  # "precipitation",
  "mean_ppt",
  "ppt_anomaly",
  "meanPptDay_anomaly",
  "intensity",
  "ugi",
  "slope",
  "distToRiver",
  "landform",
  "twi",
  "tMean",
  "parMean",
  "potentialET",
  "sand",
  "wriTCFrac"
)

sample_points_subset <- sample_points_derived %>%
  drop_na() %>%
  select(all_of(vars_to_retain))

task_gpp <- as_task_regr_st(
  x = sample_points_subset,
  id = "potential_gpp",
  target = "GPP",
  coords_as_features = FALSE,
  crs = "epsg:4326"
)

task_gpp$set_col_roles("index", roles = "space")
task_gpp$set_col_roles("year", roles = "time")

# Create learner (ranger random forest) with initial tuning values

lrn_ranger_untuned <- lrn("regr.ranger", 
                          predict_type = "response",
                          num.trees = 1001,
                          mtry.ratio = 0.5,
                          min.node.size = 100,
                          sample.fraction = 0.5
)

# Create resampling strategy

spt_cv_plan <- rsmp("sptcv_cstf", folds = 10)
# nspt_cv_plan <- rsmp("cv", folds = 10)

```

# Feature selection

We use forward feature selection with the untuned model to identify
which covariates have unique information to predict GPP. For this, we
start by building all possible single-variable models. Then, selecting
the variable that gives the best performance (lowest RMSE in this case),
we build all possible two-variable models that include this variable. We
do the same with a third variable, and so on, until the point at which
adding an additional variable fails to improve model performance.

We do this separately for models using random CV and spatio-temporal CV,
and save the optimum feature set for each.

```{r rf_feature_select, message = FALSE, results = FALSE, include = FALSE}

# Create forward feature selection with untuned model, minimising rmse

perf_msr <- msr("regr.mae")

fs_method <- fs("sequential", min_features = 1)

fs_term <- trm("stagnation_batch")

spt_feature_select <- fsi(
  task = task_gpp,
  learner = lrn_ranger_untuned,
  resampling = spt_cv_plan,
  measures = perf_msr,
  terminator = fs_term
)

# nspt_feature_select <- fsi(
#   task = task_gpp,
#   learner = lrn_ranger_untuned,
#   resampling = nspt_cv_plan,
#   measure = perf_msr,
#   terminator = fs_term
# )

# Identify optimal feature set for each resampling strategy and store
# Time ~ 10-12 hours

set.seed(123)

tic()
progressr::with_progress(
  spt_feature_set <- fs_method$optimize(spt_feature_select)
)
toc()
# beep(3)

write_rds(spt_feature_select, here("results", "rds", "spt_feature_selector.rds"))
write_rds(spt_feature_set, here("results", "rds", "spt_features.rds"))
rm(spt_feature_select, spt_feature_set)
gc()
pushoverr::pushover("Feature selection complete")


# set.seed(456)
# tic()
# progressr::with_progress(
#   nspt_feature_set <- fs_method$optimize(nspt_feature_select)
# )
# toc()
# # beep(3)
# 
# write_rds(nspt_feature_select, here("results", "rds", "nspt_feature_selector.rds"))
# write_rds(nspt_feature_set, here("results", "rds", "nspt_features.rds"))
# rm(nspt_feature_set)
# gc()


```

# Hyper-parameter optimisation (model tuning)

Now we have identified relevant features (and, hopefully, excluded those
with minimal predictive power), we can use that feature set to tune
hyperparameters for a new set of models.

We create two new tasks, each with the variables selected from the
previous step. Then we define a new learner allowing for tuning of
hyperparameters. Finally, we pass the learner and task to an auto-tuner
object for each of the two CV methods. We use a random search to
identify hyperparameters that give decent performance.

```{r rf_tuning_prep, include = FALSE}

# Load feature sets and compare best performance
spt_feature_set <- read_rds(here("results", "rds", "spt_features.rds"))
# nspt_feature_set <- read_rds(here("results", "rds", "nspt_features.rds"))
spt_feature_select <- read_rds(here("results", "rds", "spt_feature_selector.rds"))
# nspt_feature_select <- read_rds(here("results", "rds", "nspt_feature_selector.rds"))

spt_feature_set
# nspt_feature_set

# Create new tasks using features selected above
task_gpp_spt <- task_gpp$clone()
task_gpp_spt$select(unlist(spt_feature_set$features))

# task_gpp_nspt <- task_gpp$clone()
# task_gpp_nspt$select(unlist(nspt_feature_set$features))

# Define new learners for tuning

lrn_ranger_tuned_spt <- lrn(
  "regr.ranger", 
  predict_type = "response",
  quantreg = TRUE,
  keep.inbag = TRUE,
  importance = "permutation",
  num.trees = 1001,
  mtry.ratio = to_tune(p_dbl(0, 1)),
  min.node.size = to_tune(p_int(100, 10000, logscale = TRUE)),
  sample.fraction = to_tune(p_dbl(0.1, 0.9))
)

# lrn_ranger_tuned_nspt <- lrn(
#   "regr.ranger", 
#   predict_type = "response",
#   quantreg = TRUE,
#   keep.inbag = TRUE,
#   importance = "permutation",
#   num.trees = 1001,
#   mtry.ratio = to_tune(p_dbl(0, 1)),
#   min.node.size = to_tune(p_int(100, 10000, logscale = TRUE)),
#   sample.fraction = to_tune(p_dbl(0.1, 0.9))
# )

set_threads(lrn_ranger_tuned_spt, 2)
# set_threads(lrn_ranger_tuned_nspt, 2)

# Define auto-tuner objects for each model

random_tuner <- tnr("random_search")

perf_msr <- msr("regr.rmse")

at_spt <- auto_tuner(
  tuner = random_tuner,
  learner = lrn_ranger_tuned_spt,
  resampling = spt_cv_plan,
  measure = perf_msr,
  term_evals = 25
)

# at_nspt <- auto_tuner(
#   tuner = random_tuner,
#   learner = lrn_ranger_tuned_nspt,
#   resampling = nspt_cv_plan,
#   measure = perf_msr,
#   term_evals = 25
# )

```



```{r rf_tuning, include=FALSE}
# Spatial model

set.seed(789)

tic()
rf_tuned_spt <- at_spt$train(task_gpp_spt)
toc()
# beep(3)

write_rds(rf_tuned_spt, here("results", "rds", "rf_tuned_spt.rds"))

# rm(rf_tuned_spt)
# gc()

# # Non-spatial RF model (random CV)
# 
# set.seed(987)
# 
# tic()
# rf_tuned_nspt <-at_nspt$train(task_gpp_nspt)
# toc()
# # beep(3)
# 
# write_rds(rf_tuned_nspt, here("results", "rds", "rf_tuned_nspt.rds"))
# 
# rm(rf_tuned_nspt)
# gc()



```

Performance assessment of spatial and non-spatial models

```{r performance}

rf_tuned_spt <- read_rds(here("results", "rds", "rf_tuned_spt.rds"))
# rf_tuned_nspt <- read_rds(here("results", "rds", "rf_tuned_nspt.rds"))

# Measures to evaluate model performance
measures <- msrs(c("regr.mae", "regr.rmse", "regr.rsq"))

rf_predictions_spt <- rf_tuned_spt$predict(task_gpp_spt)
# rf_predictions_nspt <- rf_tuned_nspt$predict(task_gpp_nspt)

rf_predictions_spt$score(measures)
# rf_predictions_nspt$score(measures)

# Predicted quantile values
rf_predictions_spt_qu <- predict(rf_tuned_spt$learner$model, task_gpp_spt$data(), type = "quantiles", quantiles = 0.9)
# rf_predictions_nspt_qu <- predict(rf_tuned_nspt$learner$model, task_gpp_nspt$data(), type = "quantiles", quantiles = 0.9)

data_with_quantiles_spt <- task_gpp_spt$data() %>%
  bind_cols(rf_predictions_spt_qu$predictions) %>%
  rename(quantile_pred = "quantile= 0.9") %>%
  mutate(quantile_pred = quantile_pred / 1000,
         GPP = GPP / 1000)
# data_with_quantiles_nspt <- task_gpp_nspt$data() %>%
#   bind_cols(rf_predictions_nspt_qu$predictions) %>%
#   rename(quantile_pred = "quantile= 0.9")

density_breaks <- c(1, 10, 100, 1000, 10000)

density_plot <- ggplot(data_with_quantiles_spt, aes(x = quantile_pred, y = GPP)) +
  geom_hex(bins = 60) +
  scale_fill_viridis_c(direction = 1, trans = "log", breaks = density_breaks) +
  geom_abline(slope = seq(0.2, 1, 0.2), intercept = 0, colour = "grey", lwd = 0.8, linetype = "longdash") +
  geom_abline(slope = 1, intercept = 0, colour = "grey", lwd = 1.6) +
  theme_classic() +
  theme(legend.position = c(0.18, 0.75), legend.key.height = unit(1.5, "cm"),
        axis.title = element_text(size = 24),
        axis.text = element_text(size = 16),
        legend.title = element_text(size = 20),
        legend.text = element_text(size = 16),
        legend.background = element_rect(fill = "transparent")) +
  xlim(-0.01, 15) +
  ylim(-0.01, 15) +
  labs(x = expression(atop("Potential GPP", (kg~C~m^-2~yr^-1))),
       y = expression(atop("Actual GPP", (kg~C~m^-2~yr^-1))),
       fill = "Number of points")

ggsave(
  here("results", "figures", "rf_density_plot2.png"),
  density_plot,
  width = 24, height = 24, units = "cm", dpi = 250
)

```


# Variable importance:

```{r importance}

# Variable importance

importance_spt <- rf_tuned_spt$learner$importance()

static_vars <- c("mean_ppt", "slope", "distToRiver", "landform", "twi", "sand", "wriTCFrac")

importance_spt_df <- tibble(variable = names(importance_spt),
                            importance = as.vector(importance_spt)) %>%
  mutate(type = ifelse(variable %in% static_vars, "Static", "Dynamic"))

importance_plot <- ggplot(
  importance_spt_df,
  aes(x = importance, y = reorder(variable, importance), colour = type)) +
  geom_point(size = 6) +
  theme_bw() +
  scale_colour_manual(values = c("darkblue", "red")) +
  labs(x = "Variable importance", y = "Variable", colour = "Variable type") +
  theme(axis.title = element_text(size = 20),
        axis.text = element_text(size = 16),
        legend.title = element_text(size = 20),
        legend.text = element_text(size = 16))
  


```


