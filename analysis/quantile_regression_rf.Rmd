---
title: "Quantile Regression of Rangeland Productivity - Quantile Regression Forests"
output: html_notebook
author: Guy Lomax
date: 2023-06-19
---

This Notebook conducts quantile regression on rangeland productivity data
using Quantile Regression Forests (QRF), modelling annual gross primary
productivity (GPP) as a function of environmental variables


```{r setup, include = FALSE}

# Data handling
library(tidyverse)
library(sf)
library(here)

# Analysis
library(mlr3verse)
library(mlr3spatiotempcv)
library(ranger)
library(future)
library(tictoc)

# Visualisation
# library(tmap)

# Parallelisation
nc <- availableCores() - 8 
plan(multisession, workers = nc)

```


```{r load, include = FALSE}

# Country boundaries
countries <- st_read(here("data", "raw", "vector", "natural_earth",
                          "ne_110m_admin_0_countries_fixed.shp"))

ke_tz <- countries %>% filter(NAME %in% c("Kenya", "Tanzania"))

# Sample point data
sample_points_raw <- st_read(here("data", "raw", "vector", "coarse",
                                  "coarseScaleSample.geojson"))

twi <- read_csv(here("data", "processed", "csv", "twi_sample.csv"))

sample_points_ppt <- st_read(here("data", "raw", "vector", "coarse",
                                  "pptDistributionSample.geojson"))

# Precipitation distribution metrics

ppt_distribution <- st_read(here("data", "raw", "vector", "coarse",
                                 "pptDistributionSample.geojson")) %>%
    dplyr::select(-c("allRangelands", "grassland", "igbp", "shrubland", "trees", "geometry")) %>%
    st_drop_geometry() %>%
    pivot_longer(cols = !id, names_to = "colname", values_to = "value") %>%
    separate_wider_delim(colname, "_", names = c("variable", "year")) %>%
    mutate(year = as.numeric(year)) %>%
    pivot_wider(names_from = "variable", values_from = "value")


```


```{r clean, include = FALSE}

yearly_vars <- c("GPP", "precipitation", "tMean", "parMean",
                 "meanPptDay", "sdii", "ugi")

sample_points <- sample_points_raw %>%
  select(-allRangelands, -igbp, -insolation, -meanT) %>%
  mutate(x = st_coordinates(geometry)[,1],
         y = st_coordinates(geometry)[,2]) %>%
  bind_cols(twi) %>%
  rename(twi = hydrosheds_twi_fd8) %>%
  pivot_longer(cols = starts_with(yearly_vars),
               names_to = "index", values_to = "value") %>%
  separate_wider_delim(index, "_", names = c("data", "year")) %>%
  pivot_wider(names_from = data, values_from = value) %>%
  mutate(year = as.numeric(year)) %>%
  left_join(ppt_distribution, by = c("id", "year")) %>%
  filter(GPP > 0) %>%
  st_as_sf(coords = c("x", "y"), crs = 4326)

# Merge tree and shrub fractions into "woody" fraction

sample_points_woody <- sample_points %>%
  mutate(woody = trees + shrubland) %>%
  select(-trees, -shrubland, -grassland)

# Calculate derived and normalised variables
sample_points_derived <- sample_points_woody %>%
  group_by(id) %>%
  mutate(mean_ppt = mean(precipitation),
         ppt_anomaly = precipitation / mean_ppt,
         mean_meanPptDay = mean(meanPptDay),
         meanPptDay_anomaly = meanPptDay - mean_meanPptDay) %>%
  ungroup()

```


# Exploratory data analysis

```{r eda, eval=FALSE, include=FALSE}

# Study area map

tm_shape(ke_tz) + tm_borders() +
  tm_shape(rangeland_ecoregions) + tm_fill(col = "ECO_NAME", style = "cat", palette = "Accent") +
  tm_shape(sample_points_raw) + tm_dots(col = "brown")

# Relative fractions of trees, shrubs, grass

ggtern(sample_points) + geom_point(aes(x = grassland, y = shrubland, z = trees), size = 0.1)

sum(sample_points$shrubland > 0.8) / nrow(sample_points)

# GPP-PPT relationship for different fractions of grassland/shrubland

sample_points_woody %>%
  mutate(grass_cat = cut_width(grassland, 0.2, boundary = 0)) %>%
ggplot(aes(x = precipitation, y = GPP)) +
  geom_point(size = 0.2, alpha = 0.2) +
  facet_wrap(~grass_cat) +
  theme_bw() +
  xlim(0, 1200) +
  ylim(0, 6000)

# Histogram of woody cover

sample_points_woody %>%
  group_by(id) %>%
  summarise(map = mean(precipitation),
            woody = mean(woody)) %>%
  ggplot(aes(x = woody)) +
  geom_histogram(fill = "palegreen", colour = "darkgreen", bins = 50) +
  theme_bw()


```

# Data preparation for RF modelling

We select variables to use for modelling, remove rows containing NA values
(the TWI raster has some pixels with NA values) and convert to a spatiotemporal
regression task. We assign point id ("id") as our index of spatial location and
"year" as our index of time.

We then choose our learner (random forest regression through the ranger package)
with quantile regression enabled, and define two alternative resampling (cross-
validation) strategies: random CV and "Leave-location-and-time-out" CV that
ensures the validation set for each fold does not overlap in either time or space
with the training set.

When defining the learner, we initially choose crude, plausible hyperparameters
of mtry.ratio = 0.5 and sample.fraction = 0.5. However, we set the initial
minimum node size to 200 (around 0.1% of the dataset) to mitigate overfitting
and to allow sufficient terminal node size that quantiles can be calculated. For
our purposes, absolute predictive accuracy is less important than retaining
enough variability in nodes to allow assessment of quantiles.


```{r data_prep}

# Create task

cols_to_retain <- c(
  "id",
  "mean_ppt",
  "ppt_anomaly",
  "meanPptDay_anomaly",
  "sdii",
  "ugi",
  # "elevation",
  "slope",
  # "distToRiver",
  "woody",
  # "landform",
  "twi",
  "tMean",
  "parMean",
  "sand",
  "slope",
  "year",
  "GPP"
)

sample_points_subset <- sample_points_derived %>%
  filter(!is.na(twi)) %>%
  select(all_of(cols_to_retain))

task_gpp <- as_task_regr_st(
  x = sample_points_subset,
  id = "potential_gpp",
  target = "GPP",
  coords_as_features = FALSE,
  crs = "epsg:4326"
)

task_gpp$set_col_roles("id", roles = "space")
task_gpp$set_col_roles("year", roles = "time")

# Create learner (ranger random forest) with initial tuning values

lrn_ranger_untuned <- lrn("regr.ranger", 
                          predict_type = "response",
                          num.trees = 1001,
                          mtry.ratio = 0.5,
                          min.node.size = 200,
                          sample.fraction = 0.5
)

set_threads(lrn_ranger_untuned, 1)

# Create resampling strategy

spt_cv_plan <- rsmp("sptcv_cstf", folds = 10)
nspt_cv_plan <- rsmp("cv", folds = 10)

```
# Feature selection

We use forward feature selection with the untuned model to identify which
covariates have unique information to predict GPP. For this, we start by
building all possible single-variable models. Then, selecting the variable that
gives the best performance (lowest RMSE in this case), we build all possible
two-variable models that include this variable. We do the same with a third
variable, and so on, until the point at which adding an additional variable
fails to improve model performance.

We do this separately for models using random CV and spatio-temporal CV, and
save the optimum feature set for each.

```{r rf_feature_select}

# Create forward feature selection with untuned model, minimising rmse

perf_msr <- msr(c("regr.rmse"))

fs_method <- fs("sequential", min_features = 1)

fs_term <- trm("stagnation_batch")

spt_feature_select <- fsi(
  task = task_gpp,
  learner = lrn_ranger_untuned,
  resampling = spt_cv_plan,
  measures = perf_msr,
  terminator = fs_term
)

nspt_feature_select <- fsi(
  task = task_gpp,
  learner = lrn_ranger_untuned,
  resampling = nspt_cv_plan,
  measure = perf_msr,
  terminator = fs_term
)

# # Identify optimal feature set for each resampling strategy and store
# # Time ~ 10-12 hours
# 
# set.seed(123)
# 
# tic()
# progressr::with_progress(
#   spt_feature_set <- fs_method$optimize(spt_feature_select)
# )
# toc()
# # beep(3)
# 
# write_rds(spt_feature_select, here("results", "rds", "spt_feature_selector.rds"))
# write_rds(spt_feature_set, here("results", "rds", "spt_features.rds"))
# rm(spt_feature_select, spt_feature_set)
# gc()
# 
# set.seed(456)
# tic()
# progressr::with_progress(
#   nspt_feature_set <- fs_method$optimize(nspt_feature_select)
# )
# toc()
# # beep(3)
# 
# write_rds(nspt_feature_select, here("results", "rds", "nspt_feature_selector.rds"))
# write_rds(nspt_feature_set, here("results", "rds", "nspt_features.rds"))
# rm(nspt_feature_set)
# gc()


# Load feature sets and compare best performance
spt_feature_set <- read_rds(here("results", "rds", "spt_features.rds"))
nspt_feature_set <- read_rds(here("results", "rds", "nspt_features.rds"))
spt_feature_select <- read_rds(here("results", "rds", "spt_feature_selector.rds"))
nspt_feature_select <- read_rds(here("results", "rds", "nspt_feature_selector.rds"))

spt_feature_set
nspt_feature_set

```

# Hyper-parameter optimisation (model tuning)

Now we have identified relevant features (and, hopefully, excluded those with
minimal predictive power), we can use that feature set to tune hyperparameters
for a new set of models.

We create two new tasks, each with the variables selected from the previous
step. Then we define a new learner allowing for tuning of hyperparameters.
Finally, we pass the learner and task to an auto-tuner object for each of the
two CV methods. We use a random search to identify hyperparameters that give
decent performance; we are not worried about perfection!

```{r rf_model}

# Create new tasks using features selected above
task_gpp_spt <- task_gpp$clone()
task_gpp_spt$select(unlist(spt_feature_set$features))

task_gpp_nspt <- task_gpp$clone()
task_gpp_nspt$select(unlist(nspt_feature_set$features))

# Clear memory by removing unneeded objects
rm(countries, ecoregions, rangeland_ecoregions,
   sample_points_raw, sample_points, sample_points_woody, twi,
   task_gpp)
gc()

# Define new learners for tuning

lrn_ranger_tuned_spt <- lrn(
  "regr.ranger", 
  predict_type = "response",
  quantreg = TRUE,
  keep.inbag = TRUE,
  num.trees = 1001,
  mtry.ratio = to_tune(p_dbl(0, 1)),
  min.node.size = to_tune(p_int(200, 20000, logscale = TRUE)),
  sample.fraction = to_tune(p_dbl(0.1, 0.9))
)

lrn_ranger_tuned_nspt <- lrn(
  "regr.ranger", 
  predict_type = "response",
  quantreg = TRUE,
  keep.inbag = TRUE,
  num.trees = 1001,
  mtry.ratio = to_tune(p_dbl(0, 1)),
  min.node.size = to_tune(p_int(200, 20000, logscale = TRUE)),
  sample.fraction = to_tune(p_dbl(0.1, 0.9))
)

# Define auto-tuner objects for each model

random_tuner <- tnr("random_search")

at_spt <- auto_tuner(
  method = random_tuner,
  learner = lrn_ranger_tuned_spt,
  resampling = spt_cv_plan,
  measure = perf_msr,
  term_evals = 25
)

at_nspt <- auto_tuner(
  method = random_tuner,
  learner = lrn_ranger_tuned_nspt,
  resampling = nspt_cv_plan,
  measure = perf_msr,
  term_evals = 25
)


# # Spatial model
# 
# set.seed(789)
# 
# tic()
# rf_tuned_spt <- progressr::with_progress(
#   at_spt$train(task_gpp_spt)
# )
# toc(log = T)
# # beep(3)
# 
# write_rds(rf_tuned_spt, here("results", "rds", "rf_tuned_spt.rds"))
# 
# # rm(rf_tuned_spt)
# # gc()
# 
# # Non-spatial RF model (random CV)
# 
# set.seed(987)
# 
# tic()
# rf_tuned_nspt <- progressr::with_progress(
#   at_nspt$train(task_gpp_nspt)
# )
# toc(log = T)
# # beep(3)
# 
# write_rds(rf_tuned_nspt, here("results", "rds", "rf_tuned_nspt.rds"))
# 
# rm(rf_tuned_nspt)
# gc()


```


Performance assessment of spatial and non-spatial models

```{r performance}

rf_tuned_spt <- read_rds(here("results", "rds", "rf_tuned_spt.rds"))
rf_tuned_nspt <- read_rds(here("results", "rds", "rf_tuned_nspt.rds"))

# Measures to evaluate model performance
measures <- msrs(c("regr.mae", "regr.rmse", "regr.rsq"))
measures_micro <- msrs(c("regr.mae", "regr.rmse", "regr.rsq"), average = "micro")

rf_predictions_spt <- rf_tuned_spt$predict(task_gpp_spt)
rf_predictions_nspt <- rf_tuned_nspt$predict(task_gpp_nspt)

rf_predictions_spt$score(measures)
rf_predictions_nspt$score(measures)

rf_predictions_spt$score(measures_micro)
rf_predictions_nspt$score(measures_micro)

# Plot predicted vs true values

rf_predictions_spt %>%
  ggplot(aes(x = response, y = truth)) +
  geom_point(size = 0.1, alpha = 0.1) +
  geom_abline(intercept = 0, slope = 1, colour = "red") +
  theme_classic() +
  labs(title = "With spatial CV")

rf_predictions_nspt %>%
  ggplot(aes(x = response, y = truth)) +
  geom_point(size = 0.1, alpha = 0.1) +
  geom_abline(intercept = 0, slope = 1, colour = "red") +
  theme_classic() +
  labs(title = "With random CV")

# Predicted quantile values
rf_predictions_spt_qu <- predict(rf_tuned_spt$learner$model, task_gpp_spt$data(), type = "quantiles", quantiles = 0.9)
rf_predictions_nspt_qu <- predict(rf_tuned_nspt$learner$model, task_gpp_nspt$data(), type = "quantiles", quantiles = 0.9)

data_with_quantiles_spt <- task_gpp_spt$data() %>%
  bind_cols(rf_predictions_spt_qu$predictions) %>%
  rename(quantile_pred = "quantile= 0.9")
data_with_quantiles_nspt <- task_gpp_nspt$data() %>%
  bind_cols(rf_predictions_nspt_qu$predictions) %>%
  rename(quantile_pred = "quantile= 0.9")

ggplot(data_with_quantiles_spt, aes(x = quantile_pred, y = GPP)) +
  geom_point(size = 0.1, alpha = 0.1) +
  geom_abline(slope = seq(0.2, 1, 0.2), intercept = 0, colour = "grey") +
  geom_abline(slope = 1, intercept = 0, colour = "red") +
  theme_classic() +
  xlim(0, 10000) +
  ylim(0, 10000) +
  labs(x = "Predicted 0.9 quantile",
       y = "True value",
       title = "Predicted vs actual GPP (g C m-2 yr-1)",
       subtitle = "Trained with spatiotemporal CV")#

ggplot(data_with_quantiles_nspt, aes(x = quantile_pred, y = GPP)) +
  geom_point(size = 0.1, alpha = 0.1) +
  geom_abline(slope = seq(0.2, 1, 0.2), intercept = 0, colour = "grey") +
  geom_abline(slope = 1, intercept = 0, colour = "red") +
  theme_classic() +
  xlim(0, 10000) +
  ylim(0, 10000) +
  labs(x = "Predicted 0.9 quantile",
       y = "True value",
       title = "Predicted vs actual GPP (g C m-2 yr-1)",
       subtitle = "Trained with random CV")


```

It worked! I built two models that predict the 90th percentile with some degree
of skill, and allow calculation of RPI.

Both models have similar performance, perhaps because I constrained them to
retain a minimum of 200 points in each node to minimise overfitting. I need to
check whether this works

Add predictions to task data table and calculate RPI for the sample dataset
using the model tuned with spatio-temporal LOO-CV.

```{r rpi}

sample_points_rpi <- data_with_quantiles_spt %>%
  mutate(rpi = GPP / quantile_pred) %>%
  bind_cols(sample_points_derived %>% select(id, year, woody, geometry, precipitation))

ggplot(sample_points_rpi, aes(x = as.factor(year), y = rpi)) +
  geom_boxplot(outlier.size = 0.1, outlier.alpha = 0.3) +
  theme_bw() +
  geom_hline(yintercept = 1) +
  ylim(0, 2)

sample_points_rpi %>%
  group_by(id) %>%
  summarise(mean_rpi = mean(rpi),
            mean_ppt = first(mean_ppt)) %>%
  ggplot(aes(x = mean_ppt, y = mean_rpi)) +
  geom_point(size = 0.2, alpha = 0.2) +
  geom_smooth(lwd = 0.5, method = "lm") +
  theme_classic()

cor(sample_points_rpi$mean_ppt, sample_points_rpi$rpi)

ggplot(sample_points_rpi, aes(x = precipitation, y = rpi)) +
  geom_point(size = 0.1, alpha = 0.1) +
  theme_classic() +
  ylim(0, 3)

sample_points_rpi %>%
  mutate(ppt_bin = cut_width(mean_ppt, width = 100, boundary = 0)) %>%
  group_by(ppt_bin) %>%
  summarise(bin_quantile = quantile(rpi, 0.9),
            f_above = sum(rpi > 1) / n(),
            f_below = sum(rpi < 1) / n(),
            mean_rpi = mean(rpi)) %>%
  ggplot(aes(x = ppt_bin, y = f_below)) +
  geom_point() +
  ylim(0.75, 1)

# Test 

# Plot mean RPI values spatially

sample_points_rpi_mean <- sample_points_rpi %>%
  group_by(id) %>%
  summarise(mean_rpi = mean(rpi),
            geometry = first(geometry)) %>%
  st_as_sf(crs = 4326)

library(tmap)

tm_shape(ke_tz) + tm_borders() +
  tm_shape(sample_points_rpi_mean) +
  tm_dots(col = "mean_rpi", palette = "PiYG", style = "cont", midpoint = 0.8)
```

Observations:
- Some annual variations remain in the boxplots, which implies that we haven't
yet removed all impact of climate on the model.
- There remains a weak positive correlation between mean_ppt and RPI, which may
reflect the tighter grouping (fewer very low values) at high mean PPT.
Perhaps this reflects greater resistance/less influence of dry years or land use
at higher ppt?
- There remains a line of points with fitted RPI exactly 1, but this is only ~1%
of the dataset. I think it's just a function of how RF models work.
- Strangely there remains a slight bias in the quantile regression. Around 93%
of sample points have an RPI < 1; if the regression were perfect, this should
be almost exactly 90%. This applies across the whole range of precipitation
values, too.



Predicting to raster stack:

```{r benchmark_models}

learners <- lrns(c("regr.ranger", "regr.nnet", "regr.svm", "regr.featureless"))

design = benchmark_grid(tsk_gpp, learners, cv10)

head(design)

bmr <- benchmark(design)


```



Predict quantiles for original set

```{r quantiles}

quantile_predictions <- lrn_ranger$predict(type = "quantiles", quantiles = 0.9)

```

