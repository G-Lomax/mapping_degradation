---
title: "Quantile Regression of Rangeland Productivity - Quantile Regression Forests"
output: html_notebook
author: Guy Lomax
date: 2023-06-19
editor_options: 
  markdown: 
    wrap: 72
---

This Notebook conducts quantile regression on rangeland productivity
data using Quantile Regression Forests (QRF), modelling annual gross
primary productivity (GPP) as a function of environmental variables

```{r setup, include = FALSE}

# Data handling
library(tidyverse)
library(sf)
library(here)

# Analysis
library(mlr3verse)
library(mlr3spatiotempcv)
library(ranger)
library(future)
library(tictoc)

# Visualisation
library(tmap)

# Parallelisation
nc <- availableCores() - 8
plan(multisession, workers = nc)

```

```{r load, include = FALSE}

# Country boundaries
ke_tz <- st_read(here("data", "raw", "vector", "kenya_tanzania.geojson"))

# Sample point data
sample_points_raw <- st_read(here("data", "raw", "vector", "coarseScaleSample.geojson"))

twi <- read_csv(here("data", "processed", "csv", "twi_sample.csv"))

sample_points_ppt <- st_read(here("data", "raw", "vector", "pptDistributionSample.geojson"))

# Precipitation distribution metrics

ppt_distribution <- st_read(here("data", "raw", "vector", "pptDistributionSample.geojson")) %>%
    dplyr::select(-c("id", "allRangelands", "grassland", "igbp", "shrubland", "trees", "geometry")) %>%
    st_drop_geometry() %>%
    pivot_longer(cols = !index, names_to = "colname", values_to = "value") %>%
    separate_wider_delim(colname, "_", names = c("variable", "year")) %>%
    mutate(year = as.numeric(year)) %>%
    pivot_wider(names_from = "variable", values_from = "value")


```

```{r clean, include = FALSE}

yearly_vars <- c("GPP", "precipitation", "tMean", "parMean", "potentialET",
                 "meanPptDay", "sdii", "ugi")

sample_points <- sample_points_raw %>%
  mutate(x = st_coordinates(geometry)[,1],
         y = st_coordinates(geometry)[,2]) %>%
  bind_cols(twi) %>%
  rename(twi = hydrosheds_twi_fd8) %>%
  select(-allRangelands, -igbp, -shrubland, -grassland, -trees, -ID, -id) %>%
  pivot_longer(cols = starts_with(yearly_vars),
               names_to = "var", values_to = "value") %>%
  separate_wider_delim(var, "_", names = c("data", "year")) %>%
  pivot_wider(names_from = data, values_from = value) %>%
  mutate(year = as.numeric(year)) %>%
  left_join(ppt_distribution, by = c("index", "year")) %>%
  filter(GPP > 0) %>%
  st_as_sf(coords = c("x", "y"), crs = 4326)

# Calculate derived and normalised variables
sample_points_derived <- sample_points %>%
  group_by(index) %>%
  mutate(mean_ppt = mean(precipitation),
         ppt_anomaly = precipitation / mean_ppt,
         mean_meanPptDay = mean(meanPptDay),
         meanPptDay_anomaly = meanPptDay - mean_meanPptDay) %>%
  ungroup()

```

# Exploratory data analysis

```{r eda}

# Study area map and sample points

tm_shape(ke_tz) + tm_borders() +
  tm_shape(sample_points_raw) + tm_dots(col = "brown", size = 0.01)

# Fractions of tree cover in dataset

ggplot(sample_points_derived, aes(x = wriTCFrac)) +
  geom_histogram(fill = "lightgreen", colour = "forestgreen", bins = 100) +
  theme_bw()

# GPP-PPT relationship for different fractions of grassland/shrubland

sample_points_derived %>%
  mutate(woody_cat = cut_width(wriTCFrac, 20, boundary = 0)) %>%
ggplot(aes(x = precipitation, y = GPP)) +
  geom_point(size = 0.1, alpha = 0.05) +
  facet_wrap(~woody_cat) +
  theme_bw() +
  xlim(0, 1200) +
  ylim(0, 6000) +
  labs(title = "GPP vs precipitation by tree cover fraction")



```

# Data preparation for RF modelling

We select variables to use for modelling, remove rows containing NA
values (the TWI raster has some pixels with NA values) and convert to a
spatiotemporal regression task. We assign point id ("id") as our index
of spatial location and "year" as our index of time.

We then choose our learner (random forest regression through the ranger
package) with quantile regression enabled, and define two alternative
resampling (cross- validation) strategies: random CV and
"Leave-location-and-time-out" CV that ensures the validation set for
each fold does not overlap in either time or space with the training
set.

When defining the learner, we initially choose crude, plausible
hyperparameters of mtry.ratio = 0.5 and sample.fraction = 0.5. However,
we set the initial minimum node size to 200 (around 0.1% of the dataset)
to mitigate overfitting and to allow sufficient terminal node size that
quantiles can be calculated. For our purposes, absolute predictive
accuracy is less important than retaining enough variability in nodes to
allow assessment of quantiles.

```{r data_prep}

# Create task

vars_to_retain <- c(
  "index",
  "year",
  "GPP",
  "precipitation",
  "mean_ppt",
  "ppt_anomaly",
  "meanPptDay_anomaly",
  "sdii",
  "ugi",
  "slope",
  "distToRiver",
  "landform",
  "twi",
  "tMean",
  "parMean",
  "potentialET",
  "sand",
  "wriTCFrac"
)

sample_points_subset <- sample_points_derived %>%
  drop_na() %>%
  select(all_of(vars_to_retain))

task_gpp <- as_task_regr_st(
  x = sample_points_subset,
  id = "potential_gpp",
  target = "GPP",
  coords_as_features = FALSE,
  crs = "epsg:4326"
)

task_gpp$set_col_roles("index", roles = "space")
task_gpp$set_col_roles("year", roles = "time")

# Create learner (ranger random forest) with initial tuning values

lrn_ranger_untuned <- lrn("regr.ranger", 
                          predict_type = "response",
                          num.trees = 1001,
                          mtry.ratio = 0.5,
                          min.node.size = 200,
                          sample.fraction = 0.5
)

set_threads(lrn_ranger_untuned, 4)

# Create resampling strategy

spt_cv_plan <- rsmp("sptcv_cstf", folds = 10)
nspt_cv_plan <- rsmp("cv", folds = 10)

```

# Feature selection

We use forward feature selection with the untuned model to identify
which covariates have unique information to predict GPP. For this, we
start by building all possible single-variable models. Then, selecting
the variable that gives the best performance (lowest RMSE in this case),
we build all possible two-variable models that include this variable. We
do the same with a third variable, and so on, until the point at which
adding an additional variable fails to improve model performance.

We do this separately for models using random CV and spatio-temporal CV,
and save the optimum feature set for each.

```{r rf_feature_select, message = FALSE, results = FALSE, include = FALSE}

# Create forward feature selection with untuned model, minimising rmse

perf_msr <- msr("regr.rmse")

fs_method <- fs("sequential", min_features = 1)

fs_term <- trm("stagnation_batch")

spt_feature_select <- fsi(
  task = task_gpp,
  learner = lrn_ranger_untuned,
  resampling = spt_cv_plan,
  measures = perf_msr,
  terminator = fs_term
)

nspt_feature_select <- fsi(
  task = task_gpp,
  learner = lrn_ranger_untuned,
  resampling = nspt_cv_plan,
  measure = perf_msr,
  terminator = fs_term
)

# # Identify optimal feature set for each resampling strategy and store
# # Time ~ 10-12 hours
# 
# set.seed(123)
# 
# tic()
# progressr::with_progress(
#   spt_feature_set <- fs_method$optimize(spt_feature_select)
# )
# toc()
# # beep(3)
# 
# write_rds(spt_feature_select, here("results", "rds", "spt_feature_selector.rds"))
# write_rds(spt_feature_set, here("results", "rds", "spt_features.rds"))
# rm(spt_feature_select, spt_feature_set)
# gc()
# 
# set.seed(456)
# tic()
# progressr::with_progress(
#   nspt_feature_set <- fs_method$optimize(nspt_feature_select)
# )
# toc()
# # beep(3)
# 
# write_rds(nspt_feature_select, here("results", "rds", "nspt_feature_selector.rds"))
# write_rds(nspt_feature_set, here("results", "rds", "nspt_features.rds"))
# rm(nspt_feature_set)
# gc()


```

# Hyper-parameter optimisation (model tuning)

Now we have identified relevant features (and, hopefully, excluded those
with minimal predictive power), we can use that feature set to tune
hyperparameters for a new set of models.

We create two new tasks, each with the variables selected from the
previous step. Then we define a new learner allowing for tuning of
hyperparameters. Finally, we pass the learner and task to an auto-tuner
object for each of the two CV methods. We use a random search to
identify hyperparameters that give decent performance; we are not
worried about perfection!

```{r rf_tuning_prep, include = FALSE}

# Load feature sets and compare best performance
spt_feature_set <- read_rds(here("results", "rds", "spt_features.rds"))
nspt_feature_set <- read_rds(here("results", "rds", "nspt_features.rds"))
spt_feature_select <- read_rds(here("results", "rds", "spt_feature_selector.rds"))
nspt_feature_select <- read_rds(here("results", "rds", "nspt_feature_selector.rds"))

spt_feature_set
nspt_feature_set

# Create new tasks using features selected above
task_gpp_spt <- task_gpp$clone()
task_gpp_spt$select(unlist(spt_feature_set$features))

task_gpp_nspt <- task_gpp$clone()
task_gpp_nspt$select(unlist(nspt_feature_set$features))

# Define new learners for tuning

lrn_ranger_tuned_spt <- lrn(
  "regr.ranger", 
  predict_type = "response",
  quantreg = TRUE,
  keep.inbag = TRUE,
  importance = "permutation",
  num.trees = 1001,
  mtry.ratio = to_tune(p_dbl(0, 1)),
  min.node.size = to_tune(p_int(200, 20000, logscale = TRUE)),
  sample.fraction = to_tune(p_dbl(0.1, 0.9))
)

lrn_ranger_tuned_nspt <- lrn(
  "regr.ranger", 
  predict_type = "response",
  quantreg = TRUE,
  keep.inbag = TRUE,
  importance = "permutation",
  num.trees = 1001,
  mtry.ratio = to_tune(p_dbl(0, 1)),
  min.node.size = to_tune(p_int(200, 20000, logscale = TRUE)),
  sample.fraction = to_tune(p_dbl(0.1, 0.9))
)

set_threads(lrn_ranger_tuned_spt, 2)
set_threads(lrn_ranger_tuned_nspt, 2)

# Define auto-tuner objects for each model

random_tuner <- tnr("random_search")

perf_msr <- msr("regr.rmse")

at_spt <- auto_tuner(
  tuner = random_tuner,
  learner = lrn_ranger_tuned_spt,
  resampling = spt_cv_plan,
  measure = perf_msr,
  term_evals = 25
)

at_nspt <- auto_tuner(
  tuner = random_tuner,
  learner = lrn_ranger_tuned_nspt,
  resampling = nspt_cv_plan,
  measure = perf_msr,
  term_evals = 25
)

```

```{r rf_tuning, eval = FALSE}
# Spatial model

set.seed(789)

tic()
rf_tuned_spt <- progressr::with_progress(
  at_spt$train(task_gpp_spt)
)
toc(log = T)
# beep(3)

write_rds(rf_tuned_spt, here("results", "rds", "rf_tuned_spt.rds"))

# rm(rf_tuned_spt)
# gc()

# Non-spatial RF model (random CV)

set.seed(987)

tic()
rf_tuned_nspt <- progressr::with_progress(
  at_nspt$train(task_gpp_nspt)
)
toc(log = T)
# beep(3)

write_rds(rf_tuned_nspt, here("results", "rds", "rf_tuned_nspt.rds"))

rm(rf_tuned_nspt)
gc()



```

Performance assessment of spatial and non-spatial models

```{r performance}

rf_tuned_spt <- read_rds(here("results", "rds", "rf_tuned_spt.rds"))
rf_tuned_nspt <- read_rds(here("results", "rds", "rf_tuned_nspt.rds"))

# Measures to evaluate model performance
measures <- msrs(c("regr.mae", "regr.rmse", "regr.rsq"))
measures_micro <- msrs(c("regr.mae", "regr.rmse", "regr.rsq"), average = "micro")

rf_predictions_spt <- rf_tuned_spt$predict(task_gpp_spt)
rf_predictions_nspt <- rf_tuned_nspt$predict(task_gpp_nspt)

rf_predictions_spt$score(measures)
rf_predictions_nspt$score(measures)

rf_predictions_spt$score(measures_micro)
rf_predictions_nspt$score(measures_micro)

# Plot predicted vs true values

rf_predictions_spt %>%
  ggplot(aes(x = response, y = truth)) +
  geom_point(size = 0.1, alpha = 0.1) +
  geom_abline(intercept = 0, slope = 1, colour = "red") +
  theme_classic() +
  labs(title = "With spatiotemporal CV")

rf_predictions_nspt %>%
  ggplot(aes(x = response, y = truth)) +
  geom_point(size = 0.1, alpha = 0.1) +
  geom_abline(intercept = 0, slope = 1, colour = "red") +
  theme_classic() +
  labs(title = "With random CV")

# Predicted quantile values
rf_predictions_spt_qu <- predict(rf_tuned_spt$learner$model, task_gpp_spt$data(), type = "quantiles", quantiles = 0.9)
rf_predictions_nspt_qu <- predict(rf_tuned_nspt$learner$model, task_gpp_nspt$data(), type = "quantiles", quantiles = 0.9)

data_with_quantiles_spt <- task_gpp_spt$data() %>%
  bind_cols(rf_predictions_spt_qu$predictions) %>%
  rename(quantile_pred = "quantile= 0.9")
data_with_quantiles_nspt <- task_gpp_nspt$data() %>%
  bind_cols(rf_predictions_nspt_qu$predictions) %>%
  rename(quantile_pred = "quantile= 0.9")

ggplot(data_with_quantiles_spt, aes(x = quantile_pred, y = GPP)) +
  geom_point(size = 0.1, alpha = 0.1) +
  geom_abline(slope = seq(0.2, 1, 0.2), intercept = 0, colour = "grey") +
  geom_abline(slope = 1, intercept = 0, colour = "red") +
  theme_classic() +
  xlim(0, 10000) +
  ylim(0, 10000) +
  labs(x = "Predicted 0.9 quantile",
       y = "True value",
       title = "Predicted vs actual GPP (g C m-2 yr-1)",
       subtitle = "Trained with spatiotemporal CV")

ggplot(data_with_quantiles_nspt, aes(x = quantile_pred, y = GPP)) +
  geom_point(size = 0.1, alpha = 0.1) +
  geom_abline(slope = seq(0.2, 1, 0.2), intercept = 0, colour = "grey") +
  geom_abline(slope = 1, intercept = 0, colour = "red") +
  theme_classic() +
  xlim(0, 10000) +
  ylim(0, 10000) +
  labs(x = "Predicted 0.9 quantile",
       y = "True value",
       title = "Predicted vs actual GPP (g C m-2 yr-1)",
       subtitle = "Trained with random CV")

plot_theme <- theme(
    panel.background = element_rect(fill='transparent'), #transparent panel bg
    plot.background = element_rect(fill='transparent', color=NA), #transparent plot bg
    panel.grid.major = element_blank(), #remove major gridlines
    panel.grid.minor = element_blank(), #remove minor gridlines
    legend.background = element_rect(fill='transparent'), #transparent legend bg
    legend.box.background = element_rect(fill='transparent'), #transparent legend panel
    axis.title = element_text(size = 22),
    axis.text = element_text(size = 22),
    axis.ticks = element_line(linewidth = 1.2),
    axis.line = element_line(linewidth = 1.2),
    aspect.ratio = 2/3
  )

pred_vs_actual_mean <- ggplot(rf_predictions_spt, aes(x = response, y = truth)) +
  geom_point(size = 0.1, alpha = 0.1, colour = "#2D4443") +
  geom_smooth(method = "gam", colour = "#ED7A2C") +
  geom_abline(intercept = 0, slope = 1, colour = "red") +
  xlim(0,6000) + ylim(0,6000) +
  theme_classic() +
  labs(x = expression(Predicted~GPP~(g~C~m^-2~yr^-1)),
       y = expression(Actual~GPP~(g~C~m^-2~yr^-1)))

set.seed(333)
pred_vs_actual_qu <- ggplot(data_with_quantiles_spt %>% sample_frac(0.1), aes(x = quantile_pred, y = GPP)) +
  # geom_point(size = 0.4, alpha = 0.4, colour = "#2D4443") +
  geom_abline(slope = seq(0.2, 1, 0.2), intercept = 0, colour = "#ED7A2C", lwd = 0.6, linetype = "longdash") +
  # geom_abline(slope = 1, intercept = 0, colour = "#ED7A2C", lwd = 1.2) +
  theme_classic() +
  xlim(0,7000) + ylim(0,7000) +
  labs(x = "Predicted baseline GPP\n(0.9 quantile)",
       y = expression(Actual~GPP~(g~C~m^-2~yr^-1))) +
  plot_theme

pred_vs_actual_qu


ggsave(here("results", "figures", "fitted_vs_actual_qu_rf_rpi.png"),
       pred_vs_actual_qu,
       width = 36, height = 24, units = "cm", dpi = 200)

```

It worked! I built two models that predict the 90th percentile with some
degree of skill, and allow calculation of RPI.

Both models have similar performance, perhaps because I constrained them
to retain a minimum of 200 points in each node to minimise overfitting.
I need to check whether this works!

Add predictions to task data table and calculate RPI for the sample
dataset using the model tuned with LLTO-CV.

```{r rpi}

sample_points_rpi <- data_with_quantiles_spt %>%
  mutate(rpi = GPP / quantile_pred) %>%
  select(-precipitation) %>%
  bind_cols(sample_points_derived %>% select(index, year, geometry)) %>%
  mutate(precipitation = sample_points_derived$precipitation)

write_csv(sample_points_rpi,
          here("data", "processed", "csv", "sample_points_rpi.csv"))

ggplot(sample_points_rpi, aes(x = as.factor(year), y = rpi)) +
  geom_boxplot(outlier.size = 0.1, outlier.alpha = 0.3) +
  theme_bw() +
  geom_hline(yintercept = 1) +
  ylim(0, 2)

sample_points_rpi %>%
  group_by(index) %>%
  summarise(mean_rpi = mean(rpi),
            mean_ppt = first(mean_ppt)) %>%
  ggplot(aes(x = mean_ppt, y = mean_rpi)) +
  geom_point(size = 0.2, alpha = 0.2) +
  geom_smooth(lwd = 0.5, method = "lm") +
  theme_classic()

cor(sample_points_rpi$mean_ppt, sample_points_rpi$rpi)

ggplot(sample_points_rpi, aes(x = precipitation, y = rpi)) +
  geom_point(size = 0.1, alpha = 0.1) +
  theme_classic() +
  ylim(0, 3)

sample_points_rpi %>%
  mutate(ppt_bin = cut_width(mean_ppt, width = 100, boundary = 0)) %>%
  group_by(ppt_bin) %>%
  summarise(bin_quantile = quantile(rpi, 0.9),
            f_above = sum(rpi > 1) / n(),
            f_below = sum(rpi < 1) / n(),
            mean_rpi = mean(rpi)) %>%
  ggplot(aes(x = ppt_bin, y = f_below)) +
  geom_point() +
  ylim(0.75, 1)

# Test 

# Plot mean RPI values spatially

sample_points_rpi_mean <- sample_points_rpi %>%
  group_by(index) %>%
  summarise(mean_rpi = mean(rpi),
            geometry = first(geometry)) %>%
  st_as_sf(crs = 4326)

library(tmap)

tm_shape(ke_tz) + tm_borders() +
  tm_shape(sample_points_rpi_mean) +
  tm_dots(fill = "mean_rpi",
          fill.scale = tm_scale_continuous(
            limits = c(0.2, 1.2),
            midpoint = 0.7,
            values = "RdBu",
            outliers.trunc = c(T,T)
  ))


```

Observations: - Some annual variations remain in the boxplots, which
implies that we haven't yet removed all impact of climate on the
model. - There remains a weak positive correlation between mean_ppt and
RPI, which may reflect the tighter grouping (fewer very low values) at
high mean PPT. Perhaps this reflects greater resistance/less influence
of dry years or land use at higher ppt? - There remains a line of points
with fitted RPI exactly 1, but this is only \~1% of the dataset. I think
it's just a function of how RF models work. - Strangely there remains a
slight bias in the quantile regression. Around 93% of sample points have
an RPI \< 1; if the regression were perfect, this should be almost
exactly 90%. This applies across the whole range of precipitation
values, too.


